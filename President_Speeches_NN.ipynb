{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"President_Speeches_NLP.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyPApMe46TVZId1QjVDYjGBy"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"IsLrgTPDHAnx","colab_type":"code","outputId":"aa076b46-f683-42cf-fb9c-371055eb09df","executionInfo":{"status":"ok","timestamp":1581374738001,"user_tz":360,"elapsed":1092,"user":{"displayName":"Joseph Lilleberg","photoUrl":"","userId":"12613207027847372625"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7G7Q7Pk_HQQK","colab_type":"code","outputId":"3c7485cd-35af-4f79-9c99-45a2debe570a","executionInfo":{"status":"ok","timestamp":1581374743022,"user_tz":360,"elapsed":6093,"user":{"displayName":"Joseph Lilleberg","photoUrl":"","userId":"12613207027847372625"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from __future__ import absolute_import, division, print_function, unicode_literals\n","\n","%tensorflow_version 2.x\n","import tensorflow as tf\n","from tensorflow.keras import layers\n","\n","import secrets\n","import os\n","import time\n","import pickle\n","import math\n","import pandas as pd\n","import numpy as np\n","\n","# Spacy\n","import spacy\n","from spacy.matcher import Matcher\n","from spacy.lang.en import English\n","\n","# Custom Tokenizer\n","import re\n","import spacy\n","from spacy.tokenizer import Tokenizer\n","from spacy.lang.char_classes import ALPHA, ALPHA_LOWER, ALPHA_UPPER, CONCAT_QUOTES, LIST_ELLIPSES, LIST_ICONS\n","from spacy.lang.de.punctuation import _quotes\n","from spacy.util import compile_prefix_regex, compile_infix_regex, compile_suffix_regex"],"execution_count":2,"outputs":[{"output_type":"stream","text":["TensorFlow 2.x selected.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lFrcUVkikSq3","colab_type":"code","colab":{}},"source":["# https://stackoverflow.com/questions/57295996/is-it-possible-to-change-the-token-split-rules-for-a-spacy-tokenizer\n","\n","# Custom tokenizer to not split on hyphens\n","def custom_tokenizer(nlp):\n","    infixes = (\n","        LIST_ELLIPSES\n","        + LIST_ICONS\n","        + [\n","            r\"(?<=[{al}])\\.(?=[{au}])\".format(al=ALPHA_LOWER, au=ALPHA_UPPER),\n","            r\"(?<=[{a}])[,!?](?=[{a}])\".format(a=ALPHA),\n","            r'(?<=[{a}])[:<>=](?=[{a}])'.format(a=ALPHA),\n","            r\"(?<=[{a}]),(?=[{a}])\".format(a=ALPHA),\n","            r\"(?<=[{a}])([{q}\\]\\[])(?=[{a}])\".format(a=ALPHA, q=_quotes),\n","            r\"(?<=[{a}])--(?=[{a}])\".format(a=ALPHA),\n","            r\"(?<=[0-9])-(?=[0-9])\",\n","        ]\n","    )\n","\n","    infix_re = compile_infix_regex(infixes)\n","    \n","    updated_tokenizer =  Tokenizer(nlp.vocab, prefix_search=nlp.tokenizer.prefix_search,\n","                                suffix_search=nlp.tokenizer.suffix_search,\n","                                infix_finditer=infix_re.finditer,\n","                                token_match=nlp.tokenizer.token_match,\n","                                rules=nlp.Defaults.tokenizer_exceptions)\n","    \n","    return updated_tokenizer"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VeEUvkw4Gx0f","colab_type":"text"},"source":["# **Helper Functions**"]},{"cell_type":"code","metadata":{"id":"-HpDZ7ZZkVM2","colab_type":"code","colab":{}},"source":["def pattern_merger(doc):\n","    \"\"\" \n","        This will be called on the Doc object in the pipeline \n","    \"\"\"\n","    matched_spans = []\n","    matches = matcher(doc)\n","    for match_id, start, end in matches:\n","        span = doc[start:end]\n","        matched_spans.append(span)\n","    for span in matched_spans:  # merge into one token after collecting all matches\n","        span.merge()\n","    return doc"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rF4D3e3w1-Vl","colab_type":"code","colab":{}},"source":["def clean_transcript(doc):\n","    \"\"\"\n","    Given a doc, replaces all quotes with the correct double quotes token. Then merges the tokens in sentences\n","    with the correct formatting and correct puncuation placement. Finally removes all unncessary text in \n","    parenthese \n","    \"\"\"\n","    \n","    # Replace quotation tokens\n","    transcript_quotes = []\n","    left_quote = True\n","    for token in doc:\n","        if token.text == '\"':\n","            if left_quote:\n","                transcript_quotes += ['“']\n","                left_quote = False\n","            else:\n","                transcript_quotes += ['”']\n","                left_quote = True\n","        else:\n","            transcript_quotes += [token.text]  \n","    \n","    transcript_quotes = nlp(' '.join(transcript_quotes))\n","    \n","    punctuation_marks = ['.', ',','?','!',':',';', ',']\n","    \n","    # Merge tokens into correct placement\n","    transcript_punctuation = \"\"\n","    for i, token in enumerate(transcript_quotes):\n","        if i < len(transcript_quotes) - 1:\n","            if transcript_quotes[i+1].text in punctuation_marks:\n","                transcript_punctuation += token.text\n","            else:\n","                transcript_punctuation += token.text + ' '\n","    transcript_punctuation\n","    \n","    # Merge sentences using quotation boundaries \n","    new_transcript = transcript_punctuation.replace(' ..', '.')\n","    new_transcript = new_transcript.split(' “ ')\n","    new_transcript = ' “'.join(new_transcript)\n","    new_transcript = new_transcript.split(' ” ')\n","    new_transcript = '” '.join(new_transcript)\n","    \n","    # Removes all unncessary text in  parenthese \n","    new_transcript = new_transcript.replace('( Applause. )', '')\n","    new_transcript = new_transcript.replace('( Applause )', '')\n","    new_transcript = new_transcript.replace('(Applause.)', '')\n","    new_transcript = new_transcript.replace('(Laughs.)', '')\n","    new_transcript = new_transcript.replace('(Laughter.)', '')\n","    new_transcript = new_transcript.replace('(LAUGHTER)', '')\n","    new_transcript = new_transcript.replace('(APPLAUSE)', '')\n","    new_transcript = new_transcript.replace('( APPLAUSE )', '')\n","    new_transcript = new_transcript.replace('(laughter.)', '')\n","    new_transcript = new_transcript.replace('(TRANSLATION) ', '')\n","    new_transcript = re.sub(' -{2,}', '', new_transcript)\n","\n","    return new_transcript"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TP0CIsihkVjk","colab_type":"code","colab":{}},"source":["def remove_excess_spaces_component(doc):\n","    \"\"\" Removes all excess spaces to a single space \"\"\"\n","    filtered_text = ' '.join([token.text for token in doc if not token.is_space])\n","    return pattern_merger(nlp.make_doc(filtered_text))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zu8g82gM27wa","colab_type":"code","colab":{}},"source":["def avg_transcript_length(president):\n","  \"\"\" Returns the average length of transcripts for a specified president \"\"\"\n","  lengths = []\n","  for row in speeches[speeches.President == 'Barack Obama'].itertuples(index=False):\n","    lengths.append(len(row[5]))\n","  avg_length = math.floor(sum(lengths)/len(lengths))\n","  return avg_length"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kklq6159G1ek","colab_type":"text"},"source":["## **Load Data**"]},{"cell_type":"code","metadata":{"id":"x04k49mUej3i","colab_type":"code","colab":{}},"source":["# Load corpus and speeches\n","corpus = pickle.load(open(\"/content/drive/My Drive/President Speeches NLP/corpus.p\", \"rb\" ))\n","speeches = pickle.load(open(\"/content/drive/My Drive/President Speeches NLP/speeches.p\", \"rb\" ))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-UxSpnxWHYK5","colab_type":"code","colab":{}},"source":["# Get presidental text\n","president = 'Barack Obama'\n","text = corpus.loc[president].transcripts\n","\n","# Remove unnecessary paraenthesis \n","parenthesis_patterns = set(re.findall(\"\\([A-Za-z\\s\\.]+\\)\", text))\n","for pattern in parenthesis_patterns:\n","  text = re.sub(pattern, '', text)\n","  text = text.replace('()', '')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SSNi2JMTM7Yr","colab_type":"code","colab":{}},"source":["# Make Spacy doc\n","nlp = spacy.load('en_core_web_sm')\n","nlp.max_length = 1500000\n","\n","# Set tokenizer to the custom tokenizer\n","nlp.tokenizer = custom_tokenizer(nlp)\n","\n","# Pipeline\n","nlp.add_pipe(pattern_merger, first=True)  # add it right after the tokenizer\n","\n","# Matcher\n","matcher = Matcher(nlp.vocab)\n","\n","# Pattern to match possesion\n","matcher.add('match_conj_apos_s', None, [{'IS_ALPHA': True}, {'TEXT': '\\'s'}])\n","matcher.add('match_conj_not', None, [{'IS_ALPHA': True}, {'TEXT': 'n\\'t'}])\n","matcher.add('match_conj_apos_d', None, [{'IS_ALPHA': True}, {'TEXT': '\\'d'}])\n","matcher.add('match_conj_apos_ll', None, [{'IS_ALPHA': True}, {'TEXT': '\\'ll'}])\n","matcher.add('match_conj_apos_ll', None, [{'IS_ALPHA': True}, {'TEXT': '\\'m'}])\n","matcher.add('match_conj_apos_re', None, [{'IS_ALPHA': True}, {'TEXT': '\\'re'}])\n","matcher.add('match_conj_apos_ve', None, [{'IS_ALPHA': True}, {'TEXT': '\\'ve'}])\n","\n","doc = nlp(text)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jGCL85Q7HgNH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"4a15bc99-3850-4cae-8bd3-ab41d6cf4f21","executionInfo":{"status":"ok","timestamp":1581375159956,"user_tz":360,"elapsed":422962,"user":{"displayName":"Joseph Lilleberg","photoUrl":"","userId":"12613207027847372625"}}},"source":["# length of text is the number of characters in it\n","print ('Length of text: {} tokens'.format(len(text)))"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Length of text: 1153289 tokens\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RrnSQS15HjaA","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"outputId":"b5df924f-62b3-4ec8-c7a5-195557e61746","executionInfo":{"status":"ok","timestamp":1581375159956,"user_tz":360,"elapsed":422935,"user":{"displayName":"Joseph Lilleberg","photoUrl":"","userId":"12613207027847372625"}}},"source":["# Take a look at the first 100 tokens in doc\n","print(doc[:50])"],"execution_count":12,"outputs":[{"output_type":"stream","text":["To Chairman Dean and my great friend Dick Durbin; and to all my fellow citizens of this great nation; With profound gratitude and great humility, I accept your nomination for the presidency of the United States. Let me express my thanks to the historic slate of\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xEvk3-JBXhU1","colab_type":"code","outputId":"b8fc2704-f606-4e49-fa8c-87fc0c8ce258","executionInfo":{"status":"ok","timestamp":1581375159957,"user_tz":360,"elapsed":422910,"user":{"displayName":"Joseph Lilleberg","photoUrl":"","userId":"12613207027847372625"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Remove unnecessary tokens\n","vocab = sorted(set([token.text for token in doc])) # unigram\n","\n","# The unique tokens in the file\n","print ('{} unique characters'.format(len(vocab)))"],"execution_count":13,"outputs":[{"output_type":"stream","text":["11500 unique characters\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"sYntzVUe6Rpq","colab_type":"text"},"source":["# **Process Text**"]},{"cell_type":"markdown","metadata":{"id":"o7o1LClk6szt","colab_type":"text"},"source":["##### **Vectorize Text** "]},{"cell_type":"markdown","metadata":{"id":"GKsIkjZt6SX3","colab_type":"text"},"source":["Before training the model, map tokens to a numerical representation. That is, create a two lookup tables: one mapping tokens to numbers and another for numbers to tokens"]},{"cell_type":"code","metadata":{"id":"VENoOd0qVfZo","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":547},"outputId":"4a5fbc4f-6231-46dd-e417-9ba10646009d","executionInfo":{"status":"ok","timestamp":1581375159957,"user_tz":360,"elapsed":422884,"user":{"displayName":"Joseph Lilleberg","photoUrl":"","userId":"12613207027847372625"}}},"source":["# Mapping from unique tokens to indices\n","word2idx = {u:i for i, u in enumerate(vocab)}\n","idx2word = np.array(vocab)\n","\n","text_as_int = np.array([word2idx[word] for word in [token.text for token in doc]])\n","\n","# Examine mapping\n","print('Mapping:\\n--------\\n{')\n","for word,_ in zip(word2idx, range(20)):\n","    print('  {:4s}: {:3d},'.format(repr(word), word2idx[word]))\n","print('  ...\\n}\\n')\n","\n","# Show how the first 13 tokens from the text are mapped to integers\n","print ('The first 13 tokens from the text mapped to integers:\\n-----------------------------------------------------\\n{} ---- tokens mapped to int ---- > {}'.format(repr(doc[:13]), text_as_int[:13]))"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Mapping:\n","--------\n","{\n","  ' ' :   0,\n","  '   ':   1,\n","  '!' :   2,\n","  '$' :   3,\n","  \"'\" :   4,\n","  '(' :   5,\n","  ',' :   6,\n","  '-' :   7,\n","  '.' :   8,\n","  '..':   9,\n","  '...':  10,\n","  '1' :  11,\n","  '1,000':  12,\n","  '1,200':  13,\n","  '1,267':  14,\n","  '1,500':  15,\n","  '1.5':  16,\n","  '1.6':  17,\n","  '10':  18,\n","  '10,000':  19,\n","  ...\n","}\n","\n","The first 13 tokens from the text mapped to integers:\n","-----------------------------------------------------\n","To Chairman Dean and my great friend Dick Durbin; and to all ---- tokens mapped to int ---- > [ 2350   586   771  2850  7597  6033  5809   804   833   229  2850 10627\n","  2787]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"DTZ-SZBz_NLt","colab_type":"text"},"source":["# **Prediction Task**"]},{"cell_type":"markdown","metadata":{"id":"LYH-cy9R_Nzb","colab_type":"text"},"source":["The input to the model will be a sequence of tokens, and I train the model to predict the output the following token at each time step.\n","\n","Note: RNNs maintain an internal state that depends on the previously seen elements. So given all the tokens computed until this moment, what is the next token?"]},{"cell_type":"markdown","metadata":{"id":"WC13DR7QACHj","colab_type":"text"},"source":["### **Create training examples and targets**\n","\n","Divide the text into example sequences where each input sequence will contain seq_length tokens from the text.\n","\n","For each input sequence, the corresponding targets contain the same length of text, except shifted one token to the right.\n","\n","Therefore, break the text into chunks of seq_length+1. \n","\n","First use the tf.data.Dataset.from_tensor_slices function to convert the text vector into a stream of character indices."]},{"cell_type":"code","metadata":{"id":"GXpz7NoCYGVS","colab_type":"code","outputId":"9737221c-4039-4217-9f40-0224e0d19233","executionInfo":{"status":"ok","timestamp":1581375160316,"user_tz":360,"elapsed":423218,"user":{"displayName":"Joseph Lilleberg","photoUrl":"","userId":"12613207027847372625"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["# The maximum length sentence we want for a single input in tokens\n","seq_length = 500\n","examples_per_epoch = len(doc)//(seq_length+1)\n","\n","# Create training examples / targets\n","token_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n","\n","# Examine first 5 items in token_dataset\n","for i in token_dataset.take(5):\n","  print(idx2word[i.numpy()])"],"execution_count":15,"outputs":[{"output_type":"stream","text":["To\n","Chairman\n","Dean\n","and\n","my\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ENGvN5urAsLd","colab_type":"text"},"source":["The batch method allows to easily convert these individual tokens to sequences of the desired size."]},{"cell_type":"code","metadata":{"id":"M1P7-zwwYaNY","colab_type":"code","outputId":"ed64aa93-aa69-4efd-e4f8-d1d17ae4a2c4","executionInfo":{"status":"ok","timestamp":1581375160317,"user_tz":360,"elapsed":423194,"user":{"displayName":"Joseph Lilleberg","photoUrl":"","userId":"12613207027847372625"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["sequences = token_dataset.batch(seq_length+1, drop_remainder=True)\n","\n","# Examine first 5 items in sequences\n","for item in sequences.take(5):\n","  print(repr(' '.join(idx2word[item.numpy()])))"],"execution_count":16,"outputs":[{"output_type":"stream","text":["\"To Chairman Dean and my great friend Dick Durbin ; and to all my fellow citizens of this great nation ; With profound gratitude and great humility , I accept your nomination for the presidency of the United States . Let me express my thanks to the historic slate of candidates who accompanied me on this journey , and especially the one who traveled the farthest , a champion for working Americans and an inspiration to my daughters and to yours , Hillary Rodham Clinton . To President Clinton , who last night made the case for change as only he can make it ; to Ted Kennedy , who embodies the spirit of service ; and to the next Vice President of the United States , Joe Biden , I thank you . I am grateful to finish this journey with one of the finest statesmen of our time , a man at ease with everyone from world leaders to the conductors on the Amtrak train he still takes home every night . To the love of my life , our next First Lady , Michelle Obama , and to Sasha and Malia , I love you so much , and be : ( 1 so proud of all of you . Four years ago , I stood before you and told you my story , of the brief union between a young man from Kenya and a young woman from Kansas who weren't well off or well known , but shared a belief that in America , their son could achieve whatever he put his mind to . It is that promise that has always set this country apart , that through hard work and sacrifice , each of us can pursue our individual dreams but still come together as one American family , to ensure that the next generation can pursue their dreams as well . That's why I stand here tonight . Because for two hundred and thirty two years , at each moment when that promise was in jeopardy , ordinary men and women , students and soldiers , farmers and teachers , nurses and janitors , found the courage to keep it alive . We meet at one of those defining moments , a moment when our nation is at war , our economy is in turmoil , and the American promise has been threatened once more . Tonight , more Americans are out of work and more are working harder for less . More of you have lost your homes and even more are watching your home values plummet . More of you have cars you can't afford to drive , credit card bills you can't afford to pay , and tuition that's beyond your reach . These challenges are not all of government's making . But the failure to respond is a direct result of a broken politics in Washington and the failed policies of George W. Bush . America , we are\"\n","\"better than these last eight years . We are a better country than this . This country is more decent than one where a woman in Ohio , on the brink of retirement , finds herself one illness away from disaster after a lifetime of hard work . This country is more generous than one where a man in Indiana has to pack up the equipment he's worked on for twenty years and watch it shipped off to China , and then chokes up as he explains how he felt like a failure when he went home to tell his family the news . We are more compassionate than a government that lets veterans sleep on our streets and families slide into poverty ; that sits on its hands while a major American city drowns before our eyes . Tonight , I say to the American people , to Democrats and Republicans and Independents across this great land , enough ! This moment , this election , is our chance to keep , in the 21st century , the American promise alive . Because next week , in Minnesota , the same party that brought you two terms of George Bush and Dick Cheney will ask this country for a third . And we are here because we love this country too much to let the next four years look like the last eight . On November 4th , we must stand up and say : “ Eight is enough . ” Now let there be no doubt . The Republican nominee , John McCain , has worn the uniform of our country with bravery and distinction , and for that we owe him our gratitude and respect . And next week , we'll also hear about those occasions when he's broken with his party as evidence that he can deliver the change that we need . But the record's clear : John McCain has voted with George Bush ninety percent of the time . Senator McCain likes to talk about judgment , but really , what does it say about your judgment when you think George Bush has been right more than ninety percent of the time ? I don't know about you , but be : ( 1 not ready to take a ten percent chance on change . The truth is , on issue after issue that would make a difference in your lives , on health care and education and the economy , Senator McCain has been anything but independent . He said that our economy has made “ great progress ” under this President . He said that the fundamentals of the economy are strong . And when one of his chief advisors , the man who wrote his economic plan , was talking about the anxiety Americans are feeling , he said that we were just suffering from a “ mental recession , ” and that we've become , and I quote , “ a\"\n","\"nation of whiners . ” A nation of whiners ? Tell that to the proud auto workers at a Michigan plant who , after they found out it was closing , kept showing up every day and working as hard as ever , because they knew there were people who counted on the brakes that they made . Tell that to the military families who shoulder their burdens silently as they watch their loved ones leave for their third or fourth or fifth tour of duty . These are not whiners . They work hard and give back and keep going without complaint . These are the Americans that I know . Now , I don't believe that Senator McCain doesn't care what's going on in the lives of Americans . I just think he doesn't know . Why else would he define middle class as someone making under five million dollars a year ? How else could he propose hundreds of billions in tax breaks for big corporations and oil companies but not one penny of tax relief to more than one hundred million Americans ? How else could he offer a health care plan that would actually tax people's benefits , or an education plan that would do nothing to help families pay for college , or a plan that would privatize Social Security and gamble your retirement ? It's not because John McCain doesn't care . It's because John McCain doesn't get it . For over two decades , he's subscribed to that old , discredited Republican philosophy , give more and more to those with the most and hope that prosperity trickles down to everyone else . In Washington , they call this the Ownership Society , but what it really means is , you're on your own . Out of work ? Tough luck . No health care ? The market will fix it . Born into poverty ? Pull yourself up by your own bootstraps , even if you don't have boots . You're on your own . Well it's time for them to own their failure . It's time for us to change America . You see , we Democrats have a very different measure of what constitutes progress in this country . We measure progress by how many people can find a job that pays the mortgage ; whether you can put a little extra money away at the end of each month so you can someday watch your child receive her college diploma . We measure progress in the 23 million new jobs that were created when Bill Clinton was President , when the average American family saw its income go up $ 7,500 instead of down $ 2,000 like it has under George Bush . We measure the strength of our economy not by the number of billionaires we have or the profits of the Fortune 500 , but by whether someone with a good idea can take a risk and\"\n","\"start a new business , or whether the waitress who lives on tips can take a day off to look after a sick kid without losing her job , an economy that honors the dignity of work . The fundamentals we use to measure economic strength are whether we are living up to that fundamental promise that has made this country great , a promise that is the only reason I am standing here tonight . Because in the faces of those young veterans who come back from Iraq and Afghanistan , I see my grandfather , who signed up after Pearl Harbor , marched in Patton's Army , and was rewarded by a grateful nation with the chance to go to college on the GI Bill . In the face of that young student who sleeps just three hours before working the night shift , I think about my mom , who raised my sister and me on her own while she worked and earned her degree ; who once turned to food stamps but was still able to send us to the best schools in the country with the help of student loans and scholarships . When I listen to another worker tell me that his factory has shut down , I remember all those men and women on the South Side of Chicago who I stood by and fought for two decades ago after the local steel plant closed . And when I hear a woman talk about the difficulties of starting her own business , I think about my grandmother , who worked her way up from the secretarial pool to middle management , despite years of being passed over for promotions because she was a woman . She's the one who taught me about hard work . She's the one who put off buying a new car or a new dress for herself so that I could have a better life . She poured everything she had into me . And although she can no longer travel , I know that she's watching tonight , and that tonight is her night as well . I don't know what kind of lives John McCain thinks that celebrities lead , but this has been mine . These are my heroes . Theirs are the stories that shaped me . And it is on their behalf that I intend to win this election and keep our promise alive as President of the United States . What is that promise ? It's a promise that says each of us has the freedom to make of our own lives what we will , but that we also have the obligation to treat each other with dignity and respect . It's a promise that says the market should reward drive and innovation and generate growth , but that businesses should live up to their responsibilities to create American jobs , look out for American workers , and play by the rules of\"\n","\"the road . Ours is a promise that says government can not solve all our problems , but what it should do is that which we can not do for ourselves , protect us from harm and provide every child a decent education ; keep our water clean and our toys safe ; invest in new schools and new roads and new science and technology . Our government should work for us , not against us . It should help us , not hurt us . It should ensure opportunity not just for those with the most money and influence , but for every American who's willing to work . That's the promise of America , the idea that we are responsible for ourselves , but that we also rise or fall as one nation ; the fundamental belief that I am my brother's keeper ; I am my sister's keeper . That's the promise we need to keep . That's the change we need right now . So let me spell out exactly what that change would mean if I am President . Change means a tax code that doesn't reward the lobbyists who wrote it , but the American workers and small businesses who deserve it . Unlike John McCain , I will stop giving tax breaks to corporations that ship jobs overseas , and I will start giving them to companies that create good jobs right here in America . I will eliminate capital gains taxes for the small businesses and the start-ups that will create the high-wage , high-tech jobs of tomorrow . I will cut taxes , cut taxes , for 95 percent of all working families . Because in an economy like this , the last thing we should do is raise taxes on the middle class . And for the sake of our economy , our security , and the future of our planet , I will set a clear goal as President : in ten years , we will finally end our dependence on oil from the Middle East . Washington's been talking about our oil addiction for the last thirty years , and John McCain has been there for twenty-six of them . In that time , he's said no to higher fuel-efficiency standards for cars , no to investments in renewable energy , no to renewable fuels . And today , we import triple the amount of oil as the day that Senator McCain took office . Now is the time to end this addiction , and to understand that drilling is a stop-gap measure , not a long term solution . Not even close . As President , I will tap our natural gas reserves , invest in clean coal technology , and find ways to safely harness nuclear power . I'll help our auto companies re tool , so that the fuel-efficient cars of the future are built right here in America . I'll make it easier for the\"\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"UDhhlf12A2oj","colab_type":"text"},"source":["For each sequence, duplicate and shift it to form the input and target text by using the map method to apply a simple function to each batch:"]},{"cell_type":"code","metadata":{"id":"pwq5qZsgco_i","colab_type":"code","colab":{}},"source":["def split_input_target(chunk):\n","    input_text = chunk[:-1]\n","    target_text = chunk[1:]\n","    return input_text, target_text\n","\n","dataset = sequences.map(split_input_target)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CLRbya8KcrI7","colab_type":"code","outputId":"39bf9893-8579-4106-fda9-47d120033851","executionInfo":{"status":"ok","timestamp":1581375160318,"user_tz":360,"elapsed":423161,"user":{"displayName":"Joseph Lilleberg","photoUrl":"","userId":"12613207027847372625"}},"colab":{"base_uri":"https://localhost:8080/","height":71}},"source":["# Examine first input data and target data\n","for input_example, target_example in  dataset.take(1):\n","  print ('Input data: ', repr(' '.join(idx2word[input_example.numpy()])))\n","  print ('Target data:', repr(' '.join(idx2word[target_example.numpy()])))"],"execution_count":18,"outputs":[{"output_type":"stream","text":["Input data:  \"To Chairman Dean and my great friend Dick Durbin ; and to all my fellow citizens of this great nation ; With profound gratitude and great humility , I accept your nomination for the presidency of the United States . Let me express my thanks to the historic slate of candidates who accompanied me on this journey , and especially the one who traveled the farthest , a champion for working Americans and an inspiration to my daughters and to yours , Hillary Rodham Clinton . To President Clinton , who last night made the case for change as only he can make it ; to Ted Kennedy , who embodies the spirit of service ; and to the next Vice President of the United States , Joe Biden , I thank you . I am grateful to finish this journey with one of the finest statesmen of our time , a man at ease with everyone from world leaders to the conductors on the Amtrak train he still takes home every night . To the love of my life , our next First Lady , Michelle Obama , and to Sasha and Malia , I love you so much , and be : ( 1 so proud of all of you . Four years ago , I stood before you and told you my story , of the brief union between a young man from Kenya and a young woman from Kansas who weren't well off or well known , but shared a belief that in America , their son could achieve whatever he put his mind to . It is that promise that has always set this country apart , that through hard work and sacrifice , each of us can pursue our individual dreams but still come together as one American family , to ensure that the next generation can pursue their dreams as well . That's why I stand here tonight . Because for two hundred and thirty two years , at each moment when that promise was in jeopardy , ordinary men and women , students and soldiers , farmers and teachers , nurses and janitors , found the courage to keep it alive . We meet at one of those defining moments , a moment when our nation is at war , our economy is in turmoil , and the American promise has been threatened once more . Tonight , more Americans are out of work and more are working harder for less . More of you have lost your homes and even more are watching your home values plummet . More of you have cars you can't afford to drive , credit card bills you can't afford to pay , and tuition that's beyond your reach . These challenges are not all of government's making . But the failure to respond is a direct result of a broken politics in Washington and the failed policies of George W. Bush . America , we\"\n","Target data: \"Chairman Dean and my great friend Dick Durbin ; and to all my fellow citizens of this great nation ; With profound gratitude and great humility , I accept your nomination for the presidency of the United States . Let me express my thanks to the historic slate of candidates who accompanied me on this journey , and especially the one who traveled the farthest , a champion for working Americans and an inspiration to my daughters and to yours , Hillary Rodham Clinton . To President Clinton , who last night made the case for change as only he can make it ; to Ted Kennedy , who embodies the spirit of service ; and to the next Vice President of the United States , Joe Biden , I thank you . I am grateful to finish this journey with one of the finest statesmen of our time , a man at ease with everyone from world leaders to the conductors on the Amtrak train he still takes home every night . To the love of my life , our next First Lady , Michelle Obama , and to Sasha and Malia , I love you so much , and be : ( 1 so proud of all of you . Four years ago , I stood before you and told you my story , of the brief union between a young man from Kenya and a young woman from Kansas who weren't well off or well known , but shared a belief that in America , their son could achieve whatever he put his mind to . It is that promise that has always set this country apart , that through hard work and sacrifice , each of us can pursue our individual dreams but still come together as one American family , to ensure that the next generation can pursue their dreams as well . That's why I stand here tonight . Because for two hundred and thirty two years , at each moment when that promise was in jeopardy , ordinary men and women , students and soldiers , farmers and teachers , nurses and janitors , found the courage to keep it alive . We meet at one of those defining moments , a moment when our nation is at war , our economy is in turmoil , and the American promise has been threatened once more . Tonight , more Americans are out of work and more are working harder for less . More of you have lost your homes and even more are watching your home values plummet . More of you have cars you can't afford to drive , credit card bills you can't afford to pay , and tuition that's beyond your reach . These challenges are not all of government's making . But the failure to respond is a direct result of a broken politics in Washington and the failed policies of George W. Bush . America , we are\"\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"i_A0yt2TBJOW","colab_type":"text"},"source":["Each index of these vectors are processed as one time step. For the input at time step 0, the model receives the index for \"To\" and trys to predict the index for \"Chairman\" as the next character. At the next timestep, it does the same thing but the RNN considers the previous step context in addition to the current input character."]},{"cell_type":"code","metadata":{"id":"Y29rk39VcrL4","colab_type":"code","outputId":"f19ad3ea-7963-4dd0-f1d2-962af5933982","executionInfo":{"status":"ok","timestamp":1581375161367,"user_tz":360,"elapsed":424185,"user":{"displayName":"Joseph Lilleberg","photoUrl":"","userId":"12613207027847372625"}},"colab":{"base_uri":"https://localhost:8080/","height":272}},"source":["for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n","    print(\"Step {:4d}\".format(i))\n","    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2word[input_idx])))\n","    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2word[target_idx])))"],"execution_count":19,"outputs":[{"output_type":"stream","text":["Step    0\n","  input: 2350 ('To')\n","  expected output: 586 ('Chairman')\n","Step    1\n","  input: 586 ('Chairman')\n","  expected output: 771 ('Dean')\n","Step    2\n","  input: 771 ('Dean')\n","  expected output: 2850 ('and')\n","Step    3\n","  input: 2850 ('and')\n","  expected output: 7597 ('my')\n","Step    4\n","  input: 7597 ('my')\n","  expected output: 6033 ('great')\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"OeZHcPlfBWBY","colab_type":"text"},"source":["## **Create training batches**"]},{"cell_type":"markdown","metadata":{"id":"IbSUIBrTBWPt","colab_type":"text"},"source":["Use tf.data to split the text into manageable sequences. But before feeding this data into the model, need to shuffle the data and pack it into batches."]},{"cell_type":"code","metadata":{"id":"xpB219l9crOX","colab_type":"code","outputId":"27570910-7324-4dce-b399-bd59f1dccad1","executionInfo":{"status":"ok","timestamp":1581375161367,"user_tz":360,"elapsed":424159,"user":{"displayName":"Joseph Lilleberg","photoUrl":"","userId":"12613207027847372625"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Batch size\n","BATCH_SIZE = 64\n","\n","# Buffer size to shuffle the dataset\n","# (TF data is designed to work with possibly infinite sequences,\n","# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n","# it maintains a buffer in which it shuffles elements).\n","BUFFER_SIZE = 10000\n","\n","dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n","\n","dataset"],"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<BatchDataset shapes: ((64, 500), (64, 500)), types: (tf.int64, tf.int64)>"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"markdown","metadata":{"id":"o3ssibbJ-S-0","colab_type":"text"},"source":["# **Build The Model**"]},{"cell_type":"markdown","metadata":{"id":"0hqKACnx-RTC","colab_type":"text"},"source":["### **Use tf.keras.Sequential to define the model.**\n","\n","**tf.keras.layers.Embedding:** The input layer. A trainable lookup table that will map the numbers of each tiken to a vector with embedding_dim dimensions;\n","\n","**tf.keras.layers.LSTM:** A type of RNN with size units=rnn_units\n","\n","tf.keras.layers.Dense: The output layer, with vocab_size outputs.\n"]},{"cell_type":"code","metadata":{"id":"sUwmMwaRcrQv","colab_type":"code","colab":{}},"source":["# Length of the vocabulary in chars\n","vocab_size = len(vocab)\n","\n","# The embedding dimension\n","embedding_dim = 256\n","\n","# Number of RNN units\n","rnn_units = 1024"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"U-BPL2L9ddfI","colab_type":"code","colab":{}},"source":["def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n","\n","  model = tf.keras.Sequential([\n","    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n","                              batch_input_shape=[batch_size, None]),\n","    tf.keras.layers.LSTM(rnn_units,\n","                        return_sequences=True,\n","                        stateful=True,\n","                        recurrent_initializer='glorot_uniform'),\n","    tf.keras.layers.Dense(vocab_size),\n","    \n","  ])\n","  return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZkOzzHBJdho5","colab_type":"code","colab":{}},"source":["model = build_model(\n","  vocab_size = len(vocab),\n","  embedding_dim=embedding_dim,\n","  rnn_units=rnn_units,\n","  batch_size=BATCH_SIZE)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IGCaFWjv_CiE","colab_type":"text"},"source":["### **Try running the model**"]},{"cell_type":"code","metadata":{"id":"6AdhIW_HdjrX","colab_type":"code","outputId":"14e57604-d4e5-49fa-d221-a106e2ddd689","executionInfo":{"status":"ok","timestamp":1581375163501,"user_tz":360,"elapsed":426248,"user":{"displayName":"Joseph Lilleberg","photoUrl":"","userId":"12613207027847372625"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Check shape of output\n","for input_example_batch, target_example_batch in dataset.take(1):\n","  example_batch_predictions = model(input_example_batch)\n","  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"],"execution_count":24,"outputs":[{"output_type":"stream","text":["(64, 500, 11500) # (batch_size, sequence_length, vocab_size)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9dFCVoRH_r6a","colab_type":"text"},"source":["Note: This model has sequence_length of 500 but can be run on any length"]},{"cell_type":"code","metadata":{"id":"V_aB6a62dlxm","colab_type":"code","outputId":"a04cdbcc-dd64-46c0-d45a-66847d02ca06","executionInfo":{"status":"ok","timestamp":1581375163502,"user_tz":360,"elapsed":426224,"user":{"displayName":"Joseph Lilleberg","photoUrl":"","userId":"12613207027847372625"}},"colab":{"base_uri":"https://localhost:8080/","height":255}},"source":["model.summary()"],"execution_count":25,"outputs":[{"output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding (Embedding)        (64, None, 256)           2944000   \n","_________________________________________________________________\n","lstm (LSTM)                  (64, None, 1024)          5246976   \n","_________________________________________________________________\n","dense (Dense)                (64, None, 11500)         11787500  \n","=================================================================\n","Total params: 19,978,476\n","Trainable params: 19,978,476\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"eqj5bhQ__9W3","colab_type":"text"},"source":["To get actual predictions from the model, then need to sample from the output distribution, to get actual character indices. This distribution is defined by the logits over the token vocabulary."]},{"cell_type":"code","metadata":{"id":"GxR8az2xdpe6","colab_type":"code","colab":{}},"source":["sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n","sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QAguCp2VCGDR","colab_type":"text"},"source":["For the first batch, at each timestep, a prediction of the next token index:"]},{"cell_type":"code","metadata":{"id":"fsQU24c9drS-","colab_type":"code","outputId":"9b689cb0-7f98-46a1-d6ca-373fa98dff96","executionInfo":{"status":"ok","timestamp":1581375163503,"user_tz":360,"elapsed":426194,"user":{"displayName":"Joseph Lilleberg","photoUrl":"","userId":"12613207027847372625"}},"colab":{"base_uri":"https://localhost:8080/","height":969}},"source":["sampled_indices"],"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 4159,  1844,  1387,  7859,  7494,  6871, 10501,  8308,  9277,\n","        3496,  1835,  7636,  5874,  1667,  9901,   328, 10354,  1190,\n","        2746,  3268,  7933,  9450,  7133,  1382,  3298,   960,  1063,\n","        7106,  5514,  7562,   937,  6106,  6499, 10442,  6552,  7027,\n","       10103,  3699,  9856,  3606, 10227,  6146,  7952,   777,  1601,\n","        3840,  3667,  9439,  8140,   425,  9850,  3892,  4400,  8250,\n","        6247,    11, 10635,  3531,  6874,  3096,  8306,  1728,  1142,\n","        7495,  4149,  2912,  9799,  3308,  2729,  1243,  5840,  1367,\n","       11459,  3696,  4960,  5895, 10880,  9857,  3360,  1735,  5844,\n","        5467,  3487, 10315, 10804, 10753,  4587,   232,  8857,  1808,\n","        3788, 11151,  5894,  5345, 10653,  9664,  2043,   966,  8819,\n","        1786,  5873,  7947,  4609,  9107,  5848,  2175,  4170,  9211,\n","        8846,  7846, 11335,  4680,  1778,  9670,  6342,  5061,  6543,\n","        5457,  2666,  8606, 10424,   103,  5079,  3271,  9148,  5419,\n","        7198,  4318, 11194, 10947,  8051,  7173,  6407,  2140,  6073,\n","        9677,   804,  4510,  1329,  7520,  4656, 10657,  1171,  6952,\n","        7641, 11365,  8001,  9426,  9218,  7378,  6004,  1467,  4460,\n","       10824, 10351,   333,  9525,  2126,  6353,  5026,  8678,  8537,\n","        2482,   686,  8877,  6752, 10408,  8238,   688,  3469,  4850,\n","        1119,   547, 10816,  5817,  7409,  5244,  6502,   885,  7388,\n","       10790,  3951,  3201,  5787,  2926,  7835,   408,  7939,  9794,\n","        5784,  3594,   416,  6895,  2526,  4865,  3386,  8627,  6932,\n","        2007,  9670,  7643,  9985, 10758,  4209, 11374,  6252, 10137,\n","        1406,  4967,  7420, 10588,  5958,  2680,   651, 10335,  4490,\n","        5632,  7735,  3524,  3342,  5303,  1612,  8196,  9311, 10398,\n","        1993,  8614,  9085,  8896,    25,  8568,  6218,  9594,  7552,\n","        2887,  8860,  5120,  5733,   286,  8997,  4245,  7281,  7005,\n","       10378,  4358,  6453, 11402,  8434,   212,  6015,  1965, 11481,\n","       10634,  5825, 10437,  6087,  2250,  7527,   707,  2556,  2501,\n","        5839,   910,  5188, 10670,  1644, 10335,  2713,  8181, 11175,\n","        4074,  7020,  7523,  1638,   363,  3822,  4732,  3068,  9481,\n","       10316,  2517, 10046,  6590,  1834,   460,  6691,   567,   484,\n","        2877,  8599,  6638,  2932,  3570,  3052,  6213,  9482,   176,\n","       11353,   117,  5684,  4090,  5710,   365,  6867,  1727,  4078,\n","        5675,    62, 10696,  6688,  5792,  6068,  1720,  8003,  9098,\n","       10050,  8524, 11473,  6439,  2371,  4669, 10788,  2727,  3651,\n","        5639,  8553,  6410,  8459, 11269,  7924,  2682,  2297,    96,\n","       11121,  4471, 10546,   788,  5727,  9223,  8208,  6547,  9221,\n","        6356,   657,  1717,  7532,  6090,  2542, 10608,  1778,  6779,\n","        1310,  2987,  8031,    23,  5747,   896, 10320,  4226,  3509,\n","        5057,  1686,  6147, 10999,  8453,  2391,  1082,  2855, 10200,\n","        5358, 10625, 10312,  7032,  2823,  1156, 10364,  5684,  6626,\n","        4859,  1022,  5632, 11228,  1746, 10485,  5218,  6103,  6161,\n","        9343,  8873,  8061,  6215,  8253, 11086,  9942, 10902,  3637,\n","        4278,  7768,  4757,  3301,  1995,  1653,  6230,  4909,  8157,\n","        2829,  5867, 10061,  8685,  5902,  1500,  9656,   883,  1913,\n","        7740,  1153,  3830,  6539,  9613,  8785,  6706,   833,  7685,\n","        4350,  2670,  2409,  4404,  5575,  8631,  1913,  5435,  3384,\n","       10554,  7541,  9943,  5026,  5597,  4167,  4872,  6430,  5981,\n","       11127,  6262,  8481, 10147,  1465,  6649,  9613,  5847, 10541,\n","        8344, 10859,  2264,  8320,  5310,  9883, 10529,   433,  9098,\n","        8730,  2537,  1529,  8703, 11191,  6879,  8162,  2822,  6931,\n","        4782,  9015,  7700,  5452,  6729,  1424,  6189,  2516,   278,\n","       11436,  7849,  5046,   632,  4664,  3343,  9048,  2371,  9269,\n","        3526,  1864,   622,  5778,  7362,   724,  1818,  7485,  5471,\n","        9659,   698,  2894,  5448,  4079])"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"markdown","metadata":{"id":"dsvfVWD6D1Wz","colab_type":"text"},"source":["Decode these to see the text predicted by this untrained model:"]},{"cell_type":"code","metadata":{"id":"WvYXeCuYdskU","colab_type":"code","outputId":"4cc67efb-4729-4b7a-dcc0-f65c21c9b066","executionInfo":{"status":"ok","timestamp":1581375163744,"user_tz":360,"elapsed":426409,"user":{"displayName":"Joseph Lilleberg","photoUrl":"","userId":"12613207027847372625"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["print(\"Input: \\n\", repr(\" \".join(idx2word[input_example_batch[0]])))\n","print()\n","print(\"Next token Predictions: \\n\", repr(\" \".join(idx2word[sampled_indices ])))"],"execution_count":28,"outputs":[{"output_type":"stream","text":["Input: \n"," \"scarcely imagined . But what has not changed is the imperative of citizenship ; that willingness of a 26-year-old deacon , or a Unitarian minister , or a young mother of five to decide they loved this country so much that they'd risk everything to realize its promise . That's what it means to love America . That's what it means to believe in America . That's what it means when we say America is exceptional . For we were born of change . We broke the old aristocracies , declaring ourselves entitled not by bloodline , but endowed by our Creator with certain inalienable rights . We secure our rights and responsibilities through a system of self government , of and by and for the people . That's why we argue and fight with so much passion and conviction because we know our efforts matter . We know America is what we make of it . Look at our history . We are Lewis and Clark and Sacajawea , pioneers who braved the unfamiliar , followed by a stampede of farmers and miners , and entrepreneurs and hucksters . That's our spirit . That's who we are . We are Sojourner Truth and Fannie Lou Hamer , women who could do as much as any man and then some . And we're Susan B. Anthony , who shook the system until the law reflected that truth . That is our character . We're the immigrants who stowed away on ships to reach these shores , the huddled masses yearning to breathe free – - Holocaust survivors , Soviet defectors , the Lost Boys of Sudan . We're the hopeful strivers who cross the Rio Grande because we want our kids to know a better life . That's how we came to be . We're the slaves who built the White House and the economy of the South . We're the ranch hands and cowboys who opened up the West , and countless laborers who laid rail , and raised skyscrapers , and organized for workers ' rights . We're the fresh-faced GIs who fought to liberate a continent . And we're the Tuskeegee Airmen , and the Navajo code talkers , and the Japanese Americans who fought for this country even as their own liberty had been denied . We're the firefighters who rushed into those buildings on 9/11 , the volunteers who signed up to fight in Afghanistan and Iraq . We're the gay Americans whose blood ran in the streets of San Francisco and New York , just as blood ran down this bridge . We are storytellers , writers , poets , artists who abhor unfairness , and despise hypocrisy , and give voice to the voiceless , and tell truths that need to be told . We're the inventors of gospel and jazz and blues , bluegrass and country , and hip-hop and rock and roll , and our very own sound with all the\"\n","\n","Next Char Predictions: \n"," \"convictions Piedmont Kids operations mobilize jacking terrible poisoning righteous budge Period nearing gals Nation's some Angeles sweat Horse agitators bias outlines scared limb Kenya birth Fever Go license fates mouth Facts guns illness teach imposing lax step chant smoked carefully stunted handed overall Deepwater Minerals coaching cents savings perceived Baucus smart come deaths plagues heat 1 todos bus janitors back poison Obviously Helping mobilized conventional appointed skimming black aftermath Inequality fuller Karzai year's changing each gather unconscionable smooth bolstering Oil function fake brutalizing suppress trumpet treat despite A-grade recessions Passover claim volatile gateway exhortation tools ship START Fifty-one really Pakistani gallstones outwardly determination reports fundamental Somebody coordinate retain receipt opaque wills diploma Ownership shock hoarding elude imploring faint-hearted adherents prospers targeting 2008 embraced bicycle resiliency extraordinary looking cure war-making uninsured partly lobby hosts Si groups shootings Dick democratic Jim monitors difference torch Hochsprung kind necessities wither page sap retiring meet graduation Llodra defended turning swaps Anointed secures She'd home eighteen pursued professional Western Conrad reconciliation intellect talked piracy Consistent broadened dose Hans California turbines frontiers messages errors illusion Energetx membership triumph competitiveness becoming fray approved one-party Baker outside skewed frank captures Baptist jolt Yes downs bootstraps prove keeps Robert shock need spilled treatment counterpart woes heavier stood La earmark middle throng gloss admit Colombia surrounding delayed fitting notable buried blundering examining Mohammed petty roads takeover Richard protections repaid rectitude 100,060,000 promising he'll services mothers anyway recipients encouraging forged Aldama rejectionism craft manifests las symptoms dance hurry workings premise 80,000 grandmother Regulations young today's frustration taxes guard Superstorm months Corps abides Williams full-time Europeans ensuring toughest Muhammed surrounding affected personalized waiver consternation laureates monstrous Mrs. Around clock discretionary autoworker scourge suppressing Yad stall income Perhaps Bin innovations Carnegies Borlaugs anxieties prosecutor inevitable architecture can audience havens scouting 50th wires 21st focal consumed footsteps As its Obamacare constitution flowed 1929 traffic innocents free-thinking groundbreaking Nuclear paid repeating stamped processing yielding humility Transcontinental dignified triple afraid ceilings flag progressive hotter pressed weeks outages admittedly Ted 2000 violated deficit thereby Denis foreigners retrained phrase importantly retools homeless Commanders Nothing moonshot guards Zika tightly Ownership international Japanese asleep parent/teacher 100 formula Eretz surest courses bullets eloquent Nevada handful until presided Turner Gray angry strives expansion titans supportive lead ambition Higher swiftness focal inducing doubts G. fitting waterways Once ten envoy gunman happen rooted recommendations pass hazards plains very span understatement catch-22 criteria obligations disparaging birthplace Rico NAACP heals drives perished amidst gained stark pushing gays Maintaining shield End Q nothing's Hickenlooper closest implement sexual ravaged insists Durbin next-generation cystic administered Ukraine debating figurehead provide Q facilities boot thick mortality spanned eighteen findings cooperate draft humanity governing violent heighten price storm Living infiltrate sexual fund theories ports unacceptable Swat politically excelled sold the-board Behind repeating racing Young Mark question wants jeopardized permits ambiguous keeping dissolve relevant no-nothingness failings institute Laura harmless Wright Ah wrapped opened elephant City diffuse blunted remedied Transcontinental rid burned Polish Chrysler fox-guarding meant Criminals Pay mistrust falling shifting Coordinating apologize fade constitutional\"\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"N5-uOWbvEFlH","colab_type":"text"},"source":["# **Train the model**\n","\n","At this point, the problem can be treated as a standard classification problem. Given the previous RNN state, and the input this time step, predict the class of the next character.\n","\n","### **Attach an optimizer, and a loss function**\n","\n","The standard tf.keras.losses.sparse_categorical_crossentropy loss function works in this case because it is applied across the last dimension of the predictions.\n","\n","Because the model returns logits, we need to set the from_logits flag."]},{"cell_type":"code","metadata":{"id":"XQvZcLBTduQS","colab_type":"code","outputId":"23360c44-3087-419b-92a5-18a31b28d6d5","executionInfo":{"status":"ok","timestamp":1581375163744,"user_tz":360,"elapsed":426384,"user":{"displayName":"Joseph Lilleberg","photoUrl":"","userId":"12613207027847372625"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["def loss(labels, logits):\n","  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n","\n","example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n","print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n","print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"],"execution_count":29,"outputs":[{"output_type":"stream","text":["Prediction shape:  (64, 500, 11500)  # (batch_size, sequence_length, vocab_size)\n","scalar_loss:       9.350142\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"SSNvjezqE6Ut","colab_type":"text"},"source":["Configure the training procedure using the tf.keras.Model.compile method. The tf.keras.optimizers.Adam with default arguments and the loss function is used."]},{"cell_type":"code","metadata":{"id":"60yHR1uHd6HJ","colab_type":"code","colab":{}},"source":["model.compile(optimizer='adam', loss=loss)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dI3POr2PFP8y","colab_type":"text"},"source":["### **Configure checkpoints**\n","\n","Use a tf.keras.callbacks.ModelCheckpoint to ensure that checkpoints are saved during training:"]},{"cell_type":"code","metadata":{"id":"I1PxNbINd8kw","colab_type":"code","colab":{}},"source":["# Directory where the checkpoints will be saved\n","checkpoint_dir = './training_checkpoints'\n","# Name of the checkpoint files\n","checkpoint_prefix = os.path.join(checkpoint_dir, \"last_ckpt\")#\"ckpt_{epoch}\")\n","\n","checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n","    filepath=checkpoint_prefix,\n","    save_weights_only=True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5Pgv-36qFfS2","colab_type":"text"},"source":["### **Execute the training**"]},{"cell_type":"code","metadata":{"id":"7WVm1Sstr8Yt","colab_type":"code","outputId":"641ac4cd-ffaf-48ae-825d-aecc341bb94f","executionInfo":{"status":"ok","timestamp":1581379951175,"user_tz":360,"elapsed":2940895,"user":{"displayName":"Joseph Lilleberg","photoUrl":"","userId":"12613207027847372625"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["EPOCHS=500\n","history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"],"execution_count":32,"outputs":[{"output_type":"stream","text":["Train for 7 steps\n","Epoch 1/500\n","7/7 [==============================] - 9s 1s/step - loss: 8.4121\n","Epoch 2/500\n","7/7 [==============================] - 8s 1s/step - loss: 6.7878\n","Epoch 3/500\n","7/7 [==============================] - 8s 1s/step - loss: 6.6970\n","Epoch 4/500\n","7/7 [==============================] - 8s 1s/step - loss: 6.6191\n","Epoch 5/500\n","7/7 [==============================] - 8s 1s/step - loss: 6.5777\n","Epoch 6/500\n","7/7 [==============================] - 8s 1s/step - loss: 6.5592\n","Epoch 7/500\n","7/7 [==============================] - 8s 1s/step - loss: 6.5503\n","Epoch 8/500\n","7/7 [==============================] - 8s 1s/step - loss: 6.5432\n","Epoch 9/500\n","7/7 [==============================] - 8s 1s/step - loss: 6.5443\n","Epoch 10/500\n","7/7 [==============================] - 9s 1s/step - loss: 6.5444\n","Epoch 11/500\n","7/7 [==============================] - 9s 1s/step - loss: 6.5416\n","Epoch 12/500\n","7/7 [==============================] - 9s 1s/step - loss: 6.5415\n","Epoch 13/500\n","7/7 [==============================] - 9s 1s/step - loss: 6.5416\n","Epoch 14/500\n","7/7 [==============================] - 9s 1s/step - loss: 6.5413\n","Epoch 15/500\n","7/7 [==============================] - 9s 1s/step - loss: 6.5340\n","Epoch 16/500\n","7/7 [==============================] - 9s 1s/step - loss: 6.5269\n","Epoch 17/500\n","7/7 [==============================] - 9s 1s/step - loss: 6.5178\n","Epoch 18/500\n","7/7 [==============================] - 10s 1s/step - loss: 6.4962\n","Epoch 19/500\n","7/7 [==============================] - 10s 1s/step - loss: 6.4580\n","Epoch 20/500\n","7/7 [==============================] - 9s 1s/step - loss: 6.4015\n","Epoch 21/500\n","7/7 [==============================] - 9s 1s/step - loss: 6.3323\n","Epoch 22/500\n","7/7 [==============================] - 9s 1s/step - loss: 6.2567\n","Epoch 23/500\n","7/7 [==============================] - 9s 1s/step - loss: 6.1798\n","Epoch 24/500\n","7/7 [==============================] - 9s 1s/step - loss: 6.1146\n","Epoch 25/500\n","7/7 [==============================] - 9s 1s/step - loss: 6.0513\n","Epoch 26/500\n","7/7 [==============================] - 10s 1s/step - loss: 5.9900\n","Epoch 27/500\n","7/7 [==============================] - 9s 1s/step - loss: 5.9356\n","Epoch 28/500\n","7/7 [==============================] - 9s 1s/step - loss: 5.8815\n","Epoch 29/500\n","7/7 [==============================] - 9s 1s/step - loss: 5.8320\n","Epoch 30/500\n","7/7 [==============================] - 10s 1s/step - loss: 5.7836\n","Epoch 31/500\n","7/7 [==============================] - 10s 1s/step - loss: 5.7388\n","Epoch 32/500\n","7/7 [==============================] - 10s 1s/step - loss: 5.6959\n","Epoch 33/500\n","7/7 [==============================] - 10s 1s/step - loss: 5.6543\n","Epoch 34/500\n","7/7 [==============================] - 10s 1s/step - loss: 5.6197\n","Epoch 35/500\n","7/7 [==============================] - 10s 1s/step - loss: 5.5774\n","Epoch 36/500\n","7/7 [==============================] - 10s 1s/step - loss: 5.5417\n","Epoch 37/500\n","7/7 [==============================] - 10s 1s/step - loss: 5.5112\n","Epoch 38/500\n","7/7 [==============================] - 10s 1s/step - loss: 5.4756\n","Epoch 39/500\n","7/7 [==============================] - 9s 1s/step - loss: 5.4431\n","Epoch 40/500\n","7/7 [==============================] - 9s 1s/step - loss: 5.4104\n","Epoch 41/500\n","7/7 [==============================] - 9s 1s/step - loss: 5.3783\n","Epoch 42/500\n","7/7 [==============================] - 9s 1s/step - loss: 5.3488\n","Epoch 43/500\n","7/7 [==============================] - 10s 1s/step - loss: 5.3190\n","Epoch 44/500\n","7/7 [==============================] - 10s 1s/step - loss: 5.2878\n","Epoch 45/500\n","7/7 [==============================] - 9s 1s/step - loss: 5.2567\n","Epoch 46/500\n","7/7 [==============================] - 9s 1s/step - loss: 5.2273\n","Epoch 47/500\n","7/7 [==============================] - 10s 1s/step - loss: 5.1957\n","Epoch 48/500\n","7/7 [==============================] - 10s 1s/step - loss: 5.1669\n","Epoch 49/500\n","7/7 [==============================] - 10s 1s/step - loss: 5.1379\n","Epoch 50/500\n","7/7 [==============================] - 10s 1s/step - loss: 5.1096\n","Epoch 51/500\n","7/7 [==============================] - 9s 1s/step - loss: 5.0795\n","Epoch 52/500\n","7/7 [==============================] - 10s 1s/step - loss: 5.0507\n","Epoch 53/500\n","7/7 [==============================] - 10s 1s/step - loss: 5.0258\n","Epoch 54/500\n","7/7 [==============================] - 10s 1s/step - loss: 4.9979\n","Epoch 55/500\n","7/7 [==============================] - 10s 1s/step - loss: 4.9679\n","Epoch 56/500\n","7/7 [==============================] - 10s 1s/step - loss: 4.9434\n","Epoch 57/500\n","7/7 [==============================] - 10s 1s/step - loss: 4.9181\n","Epoch 58/500\n","7/7 [==============================] - 10s 1s/step - loss: 4.8940\n","Epoch 59/500\n","7/7 [==============================] - 10s 1s/step - loss: 4.8699\n","Epoch 60/500\n","7/7 [==============================] - 10s 1s/step - loss: 4.8441\n","Epoch 61/500\n","7/7 [==============================] - 10s 1s/step - loss: 4.8190\n","Epoch 62/500\n","7/7 [==============================] - 9s 1s/step - loss: 4.7949\n","Epoch 63/500\n","7/7 [==============================] - 10s 1s/step - loss: 4.7674\n","Epoch 64/500\n","7/7 [==============================] - 9s 1s/step - loss: 4.7440\n","Epoch 65/500\n","7/7 [==============================] - 10s 1s/step - loss: 4.7174\n","Epoch 66/500\n","7/7 [==============================] - 9s 1s/step - loss: 4.6960\n","Epoch 67/500\n","7/7 [==============================] - 10s 1s/step - loss: 4.6720\n","Epoch 68/500\n","7/7 [==============================] - 10s 1s/step - loss: 4.6464\n","Epoch 69/500\n","7/7 [==============================] - 10s 1s/step - loss: 4.6216\n","Epoch 70/500\n","7/7 [==============================] - 9s 1s/step - loss: 4.5971\n","Epoch 71/500\n","7/7 [==============================] - 10s 1s/step - loss: 4.5756\n","Epoch 72/500\n","7/7 [==============================] - 10s 1s/step - loss: 4.5482\n","Epoch 73/500\n","7/7 [==============================] - 10s 1s/step - loss: 4.5257\n","Epoch 74/500\n","7/7 [==============================] - 9s 1s/step - loss: 4.5024\n","Epoch 75/500\n","7/7 [==============================] - 10s 1s/step - loss: 4.4781\n","Epoch 76/500\n","7/7 [==============================] - 10s 1s/step - loss: 4.4528\n","Epoch 77/500\n","7/7 [==============================] - 10s 1s/step - loss: 4.4269\n","Epoch 78/500\n","7/7 [==============================] - 10s 1s/step - loss: 4.4055\n","Epoch 79/500\n","7/7 [==============================] - 10s 1s/step - loss: 4.3810\n","Epoch 80/500\n","7/7 [==============================] - 10s 1s/step - loss: 4.3548\n","Epoch 81/500\n","7/7 [==============================] - 10s 1s/step - loss: 4.3275\n","Epoch 82/500\n","7/7 [==============================] - 9s 1s/step - loss: 4.3056\n","Epoch 83/500\n","7/7 [==============================] - 9s 1s/step - loss: 4.2808\n","Epoch 84/500\n","7/7 [==============================] - 10s 1s/step - loss: 4.2566\n","Epoch 85/500\n","7/7 [==============================] - 10s 1s/step - loss: 4.2326\n","Epoch 86/500\n","7/7 [==============================] - 10s 1s/step - loss: 4.2098\n","Epoch 87/500\n","7/7 [==============================] - 10s 1s/step - loss: 4.1861\n","Epoch 88/500\n","7/7 [==============================] - 10s 1s/step - loss: 4.1618\n","Epoch 89/500\n","7/7 [==============================] - 10s 1s/step - loss: 4.1387\n","Epoch 90/500\n","7/7 [==============================] - 10s 1s/step - loss: 4.1137\n","Epoch 91/500\n","7/7 [==============================] - 10s 1s/step - loss: 4.0925\n","Epoch 92/500\n","7/7 [==============================] - 10s 1s/step - loss: 4.0673\n","Epoch 93/500\n","7/7 [==============================] - 10s 1s/step - loss: 4.0461\n","Epoch 94/500\n","7/7 [==============================] - 10s 1s/step - loss: 4.0225\n","Epoch 95/500\n","7/7 [==============================] - 10s 1s/step - loss: 4.0025\n","Epoch 96/500\n","7/7 [==============================] - 10s 1s/step - loss: 3.9792\n","Epoch 97/500\n","7/7 [==============================] - 10s 1s/step - loss: 3.9555\n","Epoch 98/500\n","7/7 [==============================] - 10s 1s/step - loss: 3.9341\n","Epoch 99/500\n","7/7 [==============================] - 10s 1s/step - loss: 3.9078\n","Epoch 100/500\n","7/7 [==============================] - 10s 1s/step - loss: 3.8848\n","Epoch 101/500\n","7/7 [==============================] - 10s 1s/step - loss: 3.8638\n","Epoch 102/500\n","7/7 [==============================] - 10s 1s/step - loss: 3.8374\n","Epoch 103/500\n","7/7 [==============================] - 10s 1s/step - loss: 3.8153\n","Epoch 104/500\n","7/7 [==============================] - 10s 1s/step - loss: 3.7916\n","Epoch 105/500\n","7/7 [==============================] - 10s 1s/step - loss: 3.7757\n","Epoch 106/500\n","7/7 [==============================] - 10s 1s/step - loss: 3.7502\n","Epoch 107/500\n","7/7 [==============================] - 10s 1s/step - loss: 3.7262\n","Epoch 108/500\n","7/7 [==============================] - 10s 1s/step - loss: 3.7020\n","Epoch 109/500\n","7/7 [==============================] - 10s 1s/step - loss: 3.6788\n","Epoch 110/500\n","7/7 [==============================] - 10s 1s/step - loss: 3.6578\n","Epoch 111/500\n","7/7 [==============================] - 10s 1s/step - loss: 3.6364\n","Epoch 112/500\n","7/7 [==============================] - 10s 1s/step - loss: 3.6197\n","Epoch 113/500\n","7/7 [==============================] - 10s 1s/step - loss: 3.5953\n","Epoch 114/500\n","7/7 [==============================] - 10s 1s/step - loss: 3.5750\n","Epoch 115/500\n","7/7 [==============================] - 10s 1s/step - loss: 3.5543\n","Epoch 116/500\n","7/7 [==============================] - 10s 1s/step - loss: 3.5325\n","Epoch 117/500\n","7/7 [==============================] - 10s 1s/step - loss: 3.5116\n","Epoch 118/500\n","7/7 [==============================] - 10s 1s/step - loss: 3.4886\n","Epoch 119/500\n","7/7 [==============================] - 10s 1s/step - loss: 3.4683\n","Epoch 120/500\n","7/7 [==============================] - 10s 1s/step - loss: 3.4504\n","Epoch 121/500\n","7/7 [==============================] - 10s 1s/step - loss: 3.4279\n","Epoch 122/500\n","7/7 [==============================] - 10s 1s/step - loss: 3.4054\n","Epoch 123/500\n","7/7 [==============================] - 10s 1s/step - loss: 3.3890\n","Epoch 124/500\n","7/7 [==============================] - 10s 1s/step - loss: 3.3737\n","Epoch 125/500\n","7/7 [==============================] - 10s 1s/step - loss: 3.3564\n","Epoch 126/500\n","7/7 [==============================] - 10s 1s/step - loss: 3.3311\n","Epoch 127/500\n","7/7 [==============================] - 10s 1s/step - loss: 3.3129\n","Epoch 128/500\n","7/7 [==============================] - 10s 1s/step - loss: 3.2935\n","Epoch 129/500\n","7/7 [==============================] - 10s 1s/step - loss: 3.2752\n","Epoch 130/500\n","7/7 [==============================] - 10s 1s/step - loss: 3.2574\n","Epoch 131/500\n","7/7 [==============================] - 10s 1s/step - loss: 3.2392\n","Epoch 132/500\n","7/7 [==============================] - 10s 1s/step - loss: 3.2182\n","Epoch 133/500\n","7/7 [==============================] - 10s 1s/step - loss: 3.1997\n","Epoch 134/500\n","7/7 [==============================] - 10s 1s/step - loss: 3.1822\n","Epoch 135/500\n","7/7 [==============================] - 10s 1s/step - loss: 3.1657\n","Epoch 136/500\n","7/7 [==============================] - 10s 1s/step - loss: 3.1504\n","Epoch 137/500\n","7/7 [==============================] - 10s 1s/step - loss: 3.1326\n","Epoch 138/500\n","7/7 [==============================] - 10s 1s/step - loss: 3.1132\n","Epoch 139/500\n","7/7 [==============================] - 10s 1s/step - loss: 3.0973\n","Epoch 140/500\n","7/7 [==============================] - 10s 1s/step - loss: 3.0790\n","Epoch 141/500\n","7/7 [==============================] - 10s 1s/step - loss: 3.0638\n","Epoch 142/500\n","7/7 [==============================] - 10s 1s/step - loss: 3.0498\n","Epoch 143/500\n","7/7 [==============================] - 10s 1s/step - loss: 3.0375\n","Epoch 144/500\n","7/7 [==============================] - 10s 1s/step - loss: 3.0196\n","Epoch 145/500\n","7/7 [==============================] - 10s 1s/step - loss: 2.9983\n","Epoch 146/500\n","7/7 [==============================] - 10s 1s/step - loss: 2.9818\n","Epoch 147/500\n","7/7 [==============================] - 10s 1s/step - loss: 2.9647\n","Epoch 148/500\n","7/7 [==============================] - 10s 1s/step - loss: 2.9477\n","Epoch 149/500\n","7/7 [==============================] - 10s 1s/step - loss: 2.9349\n","Epoch 150/500\n","7/7 [==============================] - 10s 1s/step - loss: 2.9171\n","Epoch 151/500\n","7/7 [==============================] - 10s 1s/step - loss: 2.9005\n","Epoch 152/500\n","7/7 [==============================] - 10s 1s/step - loss: 2.8865\n","Epoch 153/500\n","7/7 [==============================] - 10s 1s/step - loss: 2.8713\n","Epoch 154/500\n","7/7 [==============================] - 10s 1s/step - loss: 2.8581\n","Epoch 155/500\n","7/7 [==============================] - 10s 1s/step - loss: 2.8400\n","Epoch 156/500\n","7/7 [==============================] - 10s 1s/step - loss: 2.8264\n","Epoch 157/500\n","7/7 [==============================] - 10s 1s/step - loss: 2.8098\n","Epoch 158/500\n","7/7 [==============================] - 10s 1s/step - loss: 2.7943\n","Epoch 159/500\n","7/7 [==============================] - 10s 1s/step - loss: 2.7815\n","Epoch 160/500\n","7/7 [==============================] - 10s 1s/step - loss: 2.7668\n","Epoch 161/500\n","7/7 [==============================] - 10s 1s/step - loss: 2.7534\n","Epoch 162/500\n","7/7 [==============================] - 10s 1s/step - loss: 2.7390\n","Epoch 163/500\n","7/7 [==============================] - 10s 1s/step - loss: 2.7237\n","Epoch 164/500\n","7/7 [==============================] - 10s 1s/step - loss: 2.7096\n","Epoch 165/500\n","7/7 [==============================] - 10s 1s/step - loss: 2.6957\n","Epoch 166/500\n","7/7 [==============================] - 9s 1s/step - loss: 2.6835\n","Epoch 167/500\n","7/7 [==============================] - 10s 1s/step - loss: 2.6676\n","Epoch 168/500\n","7/7 [==============================] - 10s 1s/step - loss: 2.6507\n","Epoch 169/500\n","7/7 [==============================] - 10s 1s/step - loss: 2.6385\n","Epoch 170/500\n","7/7 [==============================] - 10s 1s/step - loss: 2.6250\n","Epoch 171/500\n","7/7 [==============================] - 10s 1s/step - loss: 2.6114\n","Epoch 172/500\n","7/7 [==============================] - 10s 1s/step - loss: 2.5969\n","Epoch 173/500\n","7/7 [==============================] - 10s 1s/step - loss: 2.5857\n","Epoch 174/500\n","7/7 [==============================] - 10s 1s/step - loss: 2.5714\n","Epoch 175/500\n","7/7 [==============================] - 10s 1s/step - loss: 2.5594\n","Epoch 176/500\n","7/7 [==============================] - 10s 1s/step - loss: 2.5454\n","Epoch 177/500\n","7/7 [==============================] - 10s 1s/step - loss: 2.5317\n","Epoch 178/500\n","7/7 [==============================] - 10s 1s/step - loss: 2.5235\n","Epoch 179/500\n","7/7 [==============================] - 10s 1s/step - loss: 2.5119\n","Epoch 180/500\n","7/7 [==============================] - 10s 1s/step - loss: 2.4960\n","Epoch 181/500\n","7/7 [==============================] - 10s 1s/step - loss: 2.4846\n","Epoch 182/500\n","7/7 [==============================] - 10s 1s/step - loss: 2.4726\n","Epoch 183/500\n","7/7 [==============================] - 10s 1s/step - loss: 2.4558\n","Epoch 184/500\n","7/7 [==============================] - 10s 1s/step - loss: 2.4412\n","Epoch 185/500\n","7/7 [==============================] - 10s 1s/step - loss: 2.4273\n","Epoch 186/500\n","7/7 [==============================] - 10s 1s/step - loss: 2.4152\n","Epoch 187/500\n","7/7 [==============================] - 10s 1s/step - loss: 2.4021\n","Epoch 188/500\n","7/7 [==============================] - 10s 1s/step - loss: 2.3915\n","Epoch 189/500\n","7/7 [==============================] - 10s 1s/step - loss: 2.3790\n","Epoch 190/500\n","7/7 [==============================] - 10s 1s/step - loss: 2.3663\n","Epoch 191/500\n","7/7 [==============================] - 10s 1s/step - loss: 2.3511\n","Epoch 192/500\n","7/7 [==============================] - 10s 1s/step - loss: 2.3413\n","Epoch 193/500\n","7/7 [==============================] - 10s 1s/step - loss: 2.3297\n","Epoch 194/500\n","7/7 [==============================] - 10s 1s/step - loss: 2.3171\n","Epoch 195/500\n","7/7 [==============================] - 10s 1s/step - loss: 2.3077\n","Epoch 196/500\n","7/7 [==============================] - 10s 1s/step - loss: 2.2925\n","Epoch 197/500\n","7/7 [==============================] - 10s 1s/step - loss: 2.2800\n","Epoch 198/500\n","7/7 [==============================] - 10s 1s/step - loss: 2.2685\n","Epoch 199/500\n","7/7 [==============================] - 10s 1s/step - loss: 2.2558\n","Epoch 200/500\n","7/7 [==============================] - 10s 1s/step - loss: 2.2461\n","Epoch 201/500\n","7/7 [==============================] - 10s 1s/step - loss: 2.2317\n","Epoch 202/500\n","7/7 [==============================] - 10s 1s/step - loss: 2.2222\n","Epoch 203/500\n","7/7 [==============================] - 10s 1s/step - loss: 2.2074\n","Epoch 204/500\n","7/7 [==============================] - 10s 1s/step - loss: 2.1977\n","Epoch 205/500\n","7/7 [==============================] - 10s 1s/step - loss: 2.1835\n","Epoch 206/500\n","7/7 [==============================] - 10s 1s/step - loss: 2.1735\n","Epoch 207/500\n","7/7 [==============================] - 10s 1s/step - loss: 2.1627\n","Epoch 208/500\n","7/7 [==============================] - 10s 1s/step - loss: 2.1527\n","Epoch 209/500\n","7/7 [==============================] - 10s 1s/step - loss: 2.1443\n","Epoch 210/500\n","7/7 [==============================] - 10s 1s/step - loss: 2.1342\n","Epoch 211/500\n","7/7 [==============================] - 10s 1s/step - loss: 2.1273\n","Epoch 212/500\n","7/7 [==============================] - 10s 1s/step - loss: 2.1130\n","Epoch 213/500\n","7/7 [==============================] - 9s 1s/step - loss: 2.0977\n","Epoch 214/500\n","7/7 [==============================] - 10s 1s/step - loss: 2.0866\n","Epoch 215/500\n","7/7 [==============================] - 10s 1s/step - loss: 2.0729\n","Epoch 216/500\n","7/7 [==============================] - 10s 1s/step - loss: 2.0614\n","Epoch 217/500\n","7/7 [==============================] - 10s 1s/step - loss: 2.0498\n","Epoch 218/500\n","7/7 [==============================] - 10s 1s/step - loss: 2.0389\n","Epoch 219/500\n","7/7 [==============================] - 10s 1s/step - loss: 2.0288\n","Epoch 220/500\n","7/7 [==============================] - 10s 1s/step - loss: 2.0216\n","Epoch 221/500\n","7/7 [==============================] - 10s 1s/step - loss: 2.0086\n","Epoch 222/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.9980\n","Epoch 223/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.9859\n","Epoch 224/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.9746\n","Epoch 225/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.9645\n","Epoch 226/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.9547\n","Epoch 227/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.9449\n","Epoch 228/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.9383\n","Epoch 229/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.9247\n","Epoch 230/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.9148\n","Epoch 231/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.9043\n","Epoch 232/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.8931\n","Epoch 233/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.8855\n","Epoch 234/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.8765\n","Epoch 235/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.8642\n","Epoch 236/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.8537\n","Epoch 237/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.8438\n","Epoch 238/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.8336\n","Epoch 239/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.8234\n","Epoch 240/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.8152\n","Epoch 241/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.8032\n","Epoch 242/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.7934\n","Epoch 243/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.7861\n","Epoch 244/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.7756\n","Epoch 245/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.7676\n","Epoch 246/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.7542\n","Epoch 247/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.7477\n","Epoch 248/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.7369\n","Epoch 249/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.7285\n","Epoch 250/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.7186\n","Epoch 251/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.7086\n","Epoch 252/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.7050\n","Epoch 253/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.6917\n","Epoch 254/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.6803\n","Epoch 255/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.6712\n","Epoch 256/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.6626\n","Epoch 257/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.6535\n","Epoch 258/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.6477\n","Epoch 259/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.6360\n","Epoch 260/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.6267\n","Epoch 261/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.6200\n","Epoch 262/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.6088\n","Epoch 263/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.6008\n","Epoch 264/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.5902\n","Epoch 265/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.5834\n","Epoch 266/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.5728\n","Epoch 267/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.5645\n","Epoch 268/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.5565\n","Epoch 269/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.5476\n","Epoch 270/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.5399\n","Epoch 271/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.5298\n","Epoch 272/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.5205\n","Epoch 273/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.5113\n","Epoch 274/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.5013\n","Epoch 275/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.4944\n","Epoch 276/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.4896\n","Epoch 277/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.4846\n","Epoch 278/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.4769\n","Epoch 279/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.4654\n","Epoch 280/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.4567\n","Epoch 281/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.4470\n","Epoch 282/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.4372\n","Epoch 283/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.4299\n","Epoch 284/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.4257\n","Epoch 285/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.4171\n","Epoch 286/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.4068\n","Epoch 287/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.3972\n","Epoch 288/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.3874\n","Epoch 289/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.3812\n","Epoch 290/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.3704\n","Epoch 291/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.3654\n","Epoch 292/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.3582\n","Epoch 293/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.3498\n","Epoch 294/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.3441\n","Epoch 295/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.3327\n","Epoch 296/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.3256\n","Epoch 297/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.3167\n","Epoch 298/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.3100\n","Epoch 299/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.3016\n","Epoch 300/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.2957\n","Epoch 301/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.2896\n","Epoch 302/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.2821\n","Epoch 303/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.2712\n","Epoch 304/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.2674\n","Epoch 305/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.2610\n","Epoch 306/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.2567\n","Epoch 307/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.2480\n","Epoch 308/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.2385\n","Epoch 309/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.2321\n","Epoch 310/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.2221\n","Epoch 311/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.2146\n","Epoch 312/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.2068\n","Epoch 313/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.1986\n","Epoch 314/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.1925\n","Epoch 315/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.1817\n","Epoch 316/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.1781\n","Epoch 317/500\n","7/7 [==============================] - 9s 1s/step - loss: 1.1715\n","Epoch 318/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.1655\n","Epoch 319/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.1584\n","Epoch 320/500\n","7/7 [==============================] - 9s 1s/step - loss: 1.1512\n","Epoch 321/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.1439\n","Epoch 322/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.1361\n","Epoch 323/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.1294\n","Epoch 324/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.1219\n","Epoch 325/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.1151\n","Epoch 326/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.1088\n","Epoch 327/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.1036\n","Epoch 328/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.0935\n","Epoch 329/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.0880\n","Epoch 330/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.0818\n","Epoch 331/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.0737\n","Epoch 332/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.0672\n","Epoch 333/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.0596\n","Epoch 334/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.0559\n","Epoch 335/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.0473\n","Epoch 336/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.0399\n","Epoch 337/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.0352\n","Epoch 338/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.0257\n","Epoch 339/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.0209\n","Epoch 340/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.0133\n","Epoch 341/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.0154\n","Epoch 342/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.0091\n","Epoch 343/500\n","7/7 [==============================] - 10s 1s/step - loss: 1.0061\n","Epoch 344/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.9975\n","Epoch 345/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.9908\n","Epoch 346/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.9837\n","Epoch 347/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.9787\n","Epoch 348/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.9733\n","Epoch 349/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.9650\n","Epoch 350/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.9584\n","Epoch 351/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.9540\n","Epoch 352/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.9482\n","Epoch 353/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.9394\n","Epoch 354/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.9318\n","Epoch 355/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.9248\n","Epoch 356/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.9181\n","Epoch 357/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.9115\n","Epoch 358/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.9063\n","Epoch 359/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.9010\n","Epoch 360/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.8932\n","Epoch 361/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.8893\n","Epoch 362/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.8811\n","Epoch 363/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.8787\n","Epoch 364/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.8717\n","Epoch 365/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.8665\n","Epoch 366/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.8640\n","Epoch 367/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.8591\n","Epoch 368/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.8527\n","Epoch 369/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.8464\n","Epoch 370/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.8400\n","Epoch 371/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.8337\n","Epoch 372/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.8254\n","Epoch 373/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.8205\n","Epoch 374/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.8174\n","Epoch 375/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.8126\n","Epoch 376/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.8065\n","Epoch 377/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.8012\n","Epoch 378/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.7984\n","Epoch 379/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.7916\n","Epoch 380/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.7849\n","Epoch 381/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.7806\n","Epoch 382/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.7761\n","Epoch 383/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.7678\n","Epoch 384/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.7630\n","Epoch 385/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.7586\n","Epoch 386/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.7536\n","Epoch 387/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.7474\n","Epoch 388/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.7452\n","Epoch 389/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.7388\n","Epoch 390/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.7338\n","Epoch 391/500\n","7/7 [==============================] - 9s 1s/step - loss: 0.7286\n","Epoch 392/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.7247\n","Epoch 393/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.7200\n","Epoch 394/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.7152\n","Epoch 395/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.7105\n","Epoch 396/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.7046\n","Epoch 397/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.6981\n","Epoch 398/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.6928\n","Epoch 399/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.6889\n","Epoch 400/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.6865\n","Epoch 401/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.6892\n","Epoch 402/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.6843\n","Epoch 403/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.6743\n","Epoch 404/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.6671\n","Epoch 405/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.6639\n","Epoch 406/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.6567\n","Epoch 407/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.6527\n","Epoch 408/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.6473\n","Epoch 409/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.6442\n","Epoch 410/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.6397\n","Epoch 411/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.6341\n","Epoch 412/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.6311\n","Epoch 413/500\n","7/7 [==============================] - 9s 1s/step - loss: 0.6285\n","Epoch 414/500\n","7/7 [==============================] - 9s 1s/step - loss: 0.6253\n","Epoch 415/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.6232\n","Epoch 416/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.6199\n","Epoch 417/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.6130\n","Epoch 418/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.6093\n","Epoch 419/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.6043\n","Epoch 420/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.5992\n","Epoch 421/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.5938\n","Epoch 422/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.5882\n","Epoch 423/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.5865\n","Epoch 424/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.5795\n","Epoch 425/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.5746\n","Epoch 426/500\n","7/7 [==============================] - 9s 1s/step - loss: 0.5708\n","Epoch 427/500\n","7/7 [==============================] - 9s 1s/step - loss: 0.5676\n","Epoch 428/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.5654\n","Epoch 429/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.5629\n","Epoch 430/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.5552\n","Epoch 431/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.5509\n","Epoch 432/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.5488\n","Epoch 433/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.5453\n","Epoch 434/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.5395\n","Epoch 435/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.5351\n","Epoch 436/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.5321\n","Epoch 437/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.5276\n","Epoch 438/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.5233\n","Epoch 439/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.5198\n","Epoch 440/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.5169\n","Epoch 441/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.5127\n","Epoch 442/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.5117\n","Epoch 443/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.5067\n","Epoch 444/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.5018\n","Epoch 445/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.4980\n","Epoch 446/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.4934\n","Epoch 447/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.4918\n","Epoch 448/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.4873\n","Epoch 449/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.4854\n","Epoch 450/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.4811\n","Epoch 451/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.4848\n","Epoch 452/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.4785\n","Epoch 453/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.4727\n","Epoch 454/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.4695\n","Epoch 455/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.4640\n","Epoch 456/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.4607\n","Epoch 457/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.4570\n","Epoch 458/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.4533\n","Epoch 459/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.4507\n","Epoch 460/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.4485\n","Epoch 461/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.4456\n","Epoch 462/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.4446\n","Epoch 463/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.4425\n","Epoch 464/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.4410\n","Epoch 465/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.4359\n","Epoch 466/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.4309\n","Epoch 467/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.4266\n","Epoch 468/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.4242\n","Epoch 469/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.4210\n","Epoch 470/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.4147\n","Epoch 471/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.4132\n","Epoch 472/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.4102\n","Epoch 473/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.4053\n","Epoch 474/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.4036\n","Epoch 475/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.3998\n","Epoch 476/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.3973\n","Epoch 477/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.3951\n","Epoch 478/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.3907\n","Epoch 479/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.3886\n","Epoch 480/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.3858\n","Epoch 481/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.3849\n","Epoch 482/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.3791\n","Epoch 483/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.3766\n","Epoch 484/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.3747\n","Epoch 485/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.3718\n","Epoch 486/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.3683\n","Epoch 487/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.3652\n","Epoch 488/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.3628\n","Epoch 489/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.3612\n","Epoch 490/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.3572\n","Epoch 491/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.3553\n","Epoch 492/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.3523\n","Epoch 493/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.3494\n","Epoch 494/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.3468\n","Epoch 495/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.3451\n","Epoch 496/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.3422\n","Epoch 497/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.3396\n","Epoch 498/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.3358\n","Epoch 499/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.3341\n","Epoch 500/500\n","7/7 [==============================] - 10s 1s/step - loss: 0.3310\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"hnsvAUZIFn7N","colab_type":"text"},"source":["# **Generate text**\n","\n","### **Restore the latest checkpoint**\n","\n","Use a batch size of 1.\n","\n","Because of the way the RNN state is passed from timestep to timestep, the model only accepts a fixed batch size once built.\n","\n","To run the model with a different batch_size, we need to rebuild the model and restore the weights from the checkpoint."]},{"cell_type":"code","metadata":{"id":"EhZk8ko3eD1v","colab_type":"code","outputId":"584a32d3-1f51-405a-be73-7efc39d4bbf4","executionInfo":{"status":"ok","timestamp":1581379951176,"user_tz":360,"elapsed":26,"user":{"displayName":"Joseph Lilleberg","photoUrl":"","userId":"12613207027847372625"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["tf.train.latest_checkpoint(checkpoint_dir)"],"execution_count":33,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'./training_checkpoints/last_ckpt'"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"code","metadata":{"id":"KC5TQrDJeHqX","colab_type":"code","colab":{}},"source":["model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n","\n","model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n","\n","model.build(tf.TensorShape([1, None]))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hw4SWqZUeJNL","colab_type":"code","outputId":"00aea7e0-47b9-4c31-fc87-c18845cb3a48","executionInfo":{"status":"ok","timestamp":1581379951987,"user_tz":360,"elapsed":80,"user":{"displayName":"Joseph Lilleberg","photoUrl":"","userId":"12613207027847372625"}},"colab":{"base_uri":"https://localhost:8080/","height":255}},"source":["model.summary()"],"execution_count":35,"outputs":[{"output_type":"stream","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_1 (Embedding)      (1, None, 256)            2944000   \n","_________________________________________________________________\n","lstm_1 (LSTM)                (1, None, 1024)           5246976   \n","_________________________________________________________________\n","dense_1 (Dense)              (1, None, 11500)          11787500  \n","=================================================================\n","Total params: 19,978,476\n","Trainable params: 19,978,476\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"850YUsQLF3dv","colab_type":"text"},"source":["# **The prediction loop**\n","\n","The following code block generates the text:\n","\n","*   It Starts by choosing a start string, initializing the RNN state and setting the number of tokens to generate.\n","\n","*   Get the prediction distribution of the next token using the start string and the RNN state.\n","\n","*   Then, use a categorical distribution to calculate the index of the predicted token. Use this predicted token as our next input to the model.\n","\n","*   The RNN state returned by the model is fed back into the model so that it now has more context, instead than only one token. After predicting the next token, the modified RNN states are again fed back into the model, which is how it learns as it gets more context from the previously predicted characters.\n","\n"]},{"cell_type":"code","metadata":{"id":"GGG5CtFBeKnl","colab_type":"code","colab":{}},"source":["def generate_text(model, temp, start_string):\n","  # Evaluation step (generating text using the learned model)\n","\n","  # Number of characters to generate\n","  num_generate = avg_transcript_length('Barack Obama')\n","\n","  # Converting our start string to numbers (vectorizing)\n","  input_eval = [word2idx[word] for word in start_string.split()]\n","  input_eval = tf.expand_dims(input_eval, 0)\n","\n","  # Empty string to store our results\n","  text_generated = []\n","\n","  # Low temperatures results in more predictable text.\n","  # Higher temperatures results in more surprising text.\n","  # Experiment to find the best setting.\n","  temperature = temp\n","\n","  # Here batch size == 1\n","  model.reset_states()\n","  for i in range(num_generate):\n","      predictions = model(input_eval)\n","      # remove the batch dimension\n","      predictions = tf.squeeze(predictions, 0)\n","\n","      # using a categorical distribution to predict the character returned by the model\n","      predictions = predictions / temperature\n","      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n","\n","      # We pass the predicted character as the next input to the model\n","      # along with the previous hidden state\n","      input_eval = tf.expand_dims([predicted_id], 0)\n","\n","      text_generated.append(idx2word[predicted_id])\n","\n","  return (start_string + ' '.join(text_generated))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IwXUDk-_IbEk","colab_type":"code","colab":{}},"source":["def get_first_word(president):\n","  president_speeches = speeches[speeches['President'] == president].Transcript.values\n","  first_words = []\n","  for speech in president_speeches: \n","    tokens = nlp(speech[:50])\n","    first_words.append(tokens[0])\n","  first_words = list(set(map(lambda x: x.text, first_words)))\n","  return secrets.choice(first_words)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"U4KlrVQbsx9M","colab_type":"code","colab":{}},"source":["def generate_transcript(president, model, temp):\n","  first_word = get_first_word(president) + \" \"\n","  generated_text = generate_text(model, temp, start_string=first_word)\n","  generated_doc = nlp(generated_text)\n","  generated_text_final = clean_transcript(generated_doc)\n","\n","  return generated_text_final"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ml3h_3pUvbJQ","colab_type":"text"},"source":["## **Generate Text**"]},{"cell_type":"code","metadata":{"id":"0_elJStfB46b","colab_type":"code","colab":{}},"source":["generated_transcript = generate_transcript(president, model, .3)\n","generated_transcript[:10000]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qM5lWTzUxSdj","colab_type":"code","colab":{}},"source":["# Remove exccess spaces\n","generated_transcript = re.sub(' {2,}', ' ', generated_transcript)\n","\n","# Write to text file\n","with open(\"generated_transcript_Obama.txt\", \"w\") as file:\n","  file.write(generated_transcript)\n","  "],"execution_count":0,"outputs":[]}]}