{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "import scattertext as st\n",
    "\n",
    "from scattertext import RankDifference\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from itertools import combinations \n",
    "import umap\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "first_corpus = pickle.load(open(\"pickles/first_party_corpus.p\", \"rb\" ))\n",
    "second_corpus = pickle.load(open(\"pickles/second_party_corpus.p\", \"rb\" ))\n",
    "third_corpus = pickle.load(open(\"pickles/third_party_corpus.p\", \"rb\" ))\n",
    "fourth_corpus = pickle.load(open(\"pickles/fourth_party_corpus.p\", \"rb\" ))\n",
    "fifth_corpus = pickle.load(open(\"pickles/fifth_party_corpus.p\", \"rb\" ))\n",
    "sixth_corpus = pickle.load(open(\"pickles/sixth_party_corpus.p\", \"rb\" ))\n",
    "\n",
    "# Load stop words\n",
    "stop_words = list(pickle.load(open(\"pickles/stop_words.p\", \"rb\" )))\n",
    "\n",
    "# Load Spacy english model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.max_length = 1500000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bigrams(texts, bigram_mod):\n",
    "    return [bigram_mod[doc] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_trigrams(texts, bigram_mod, trigram_mod):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=dictionary)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_presidential_speech(president, corpus, filename, ngram = 1, display_output = False):\n",
    "    \n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    nlp.max_length = 1500000\n",
    "        \n",
    "    text = list(corpus[corpus.presidents == president].transcripts.values)\n",
    "    \n",
    "    words = list(sent_to_words(text))\n",
    "    \n",
    "    # Build the bigram and trigram models\n",
    "    bigram = gensim.models.Phrases(words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "    trigram = gensim.models.Phrases(bigram[words], threshold=100)  \n",
    "\n",
    "    # Faster way to get a sentence clubbed as a trigram/bigram\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "    \n",
    "    # Remove Stop Words\n",
    "    words = remove_stopwords(words)\n",
    "\n",
    "    # Form Bigrams\n",
    "    if ngram == 2:\n",
    "        words = make_bigrams(words, bigram_mod)\n",
    "    \n",
    "    if ngram == 3:\n",
    "        words = make_trigrams(words, bigram_mod, trigram_mod)\n",
    "    \n",
    "    data_lemmatized = lemmatization(words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "    \n",
    "    # Create Dictionary\n",
    "    id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "    # Create Corpus\n",
    "    texts = data_lemmatized\n",
    "\n",
    "    # Term Document Frequency\n",
    "    corpus = [id2word.doc2bow(text) for text in texts]\n",
    "    \n",
    "    # issue\n",
    "    model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=data_lemmatized, start=2, limit=40, step=6)\n",
    "    \n",
    "    limit=40; start=2; step=6;\n",
    "    x = range(start, limit, step)  \n",
    "    \n",
    "   # Print the coherence scores\n",
    "    find_first_peak = True\n",
    "    optimal_value = -1\n",
    "    optimal_topics = -1\n",
    "    mapping = list(zip(x, coherence_values))\n",
    "    for i, tup in enumerate(mapping[:-1]):\n",
    "        m, cv = tup\n",
    "\n",
    "        if find_first_peak:\n",
    "            if cv > mapping[i+1][1]:\n",
    "                find_first_peak = False\n",
    "            optimal_value = cv\n",
    "            optimal_topics = m\n",
    "\n",
    "        if display_output:\n",
    "            print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 4))\n",
    "    if display_output:\n",
    "        print(\"\\nOptimal Topic Count: {}\".format(optimal_topics))\n",
    "    \n",
    "    # Build LDA model\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                               id2word=id2word,\n",
    "                                               num_topics=optimal_topics, \n",
    "                                               random_state=100,\n",
    "                                               update_every=1,\n",
    "                                               chunksize=100,\n",
    "                                               passes=10,\n",
    "                                               alpha='auto',\n",
    "                                               per_word_topics=True)\n",
    "    \n",
    "    # Compute Perplexity\n",
    "    if display_output:\n",
    "        print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "    \n",
    "    ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=optimal_topics, id2word=id2word)\n",
    "\n",
    "    # Compute Coherence Score\n",
    "    coherence_model_ldamallet = CoherenceModel(model=ldamallet, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "    coherence_ldamallet = coherence_model_ldamallet.get_coherence()\n",
    "    if display_output:\n",
    "        print('\\nCoherence Score: ', coherence_ldamallet)\n",
    "    \n",
    "    # Visualize the topics\n",
    "    pyLDAvis.enable_notebook()\n",
    "    vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "    path = \"Visualizations/\" + filename + \"/LDA/\" + filename + \"_\" + president + \"_LDA_visualization.html\"\n",
    "    pyLDAvis.save_html(vis, path)\n",
    "\n",
    "    if display_output:\n",
    "        limit=40; start=2; step=6;\n",
    "        x = range(start, limit, step)\n",
    "        plt.plot(x, coherence_values)\n",
    "        plt.xlabel(\"Num Topics\")\n",
    "        plt.ylabel(\"Coherence score\")\n",
    "        plt.legend((\"coherence_values\"), loc='best')\n",
    "        plt.show()\n",
    "        \n",
    "    # perplexity, nCoherence, topics    \n",
    "    \n",
    "    return (lda_model.log_perplexity(corpus), coherence_ldamallet, ldamallet.show_topics(formatted=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_party_LDA_visualizations(corpus, filename, ngram = 1, display_output = False):\n",
    "    \n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    nlp.max_length = 1500000\n",
    "    \n",
    "    for party in set(corpus.party):\n",
    "    \n",
    "        text = list(corpus[corpus.party == party].transcripts.values)\n",
    "\n",
    "        words = list(sent_to_words(text))\n",
    "        \n",
    "        # Build the bigram and trigram models\n",
    "        bigram = gensim.models.Phrases(words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "        trigram = gensim.models.Phrases(bigram[words], threshold=100)  \n",
    "\n",
    "        # Faster way to get a sentence clubbed as a trigram/bigram\n",
    "        bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "        trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "        # Remove Stop Words\n",
    "        words = remove_stopwords(words)\n",
    "\n",
    "        # Form Bigrams\n",
    "        if ngram == 2:\n",
    "            words = make_bigrams(words, bigram_mod)\n",
    "\n",
    "        if ngram == 3:\n",
    "            words = make_trigrams(words, bigram_mod, trigram_mod)\n",
    "\n",
    "        data_lemmatized = lemmatization(words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "        # Create Dictionary\n",
    "        id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "        # Create Corpus\n",
    "        texts = data_lemmatized\n",
    "\n",
    "        # Term Document Frequency\n",
    "        tdf_corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "        # issue\n",
    "        model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=tdf_corpus, texts=data_lemmatized, start=2, limit=40, step=6)\n",
    "\n",
    "        limit=40; start=2; step=6;\n",
    "        x = range(start, limit, step)  \n",
    "\n",
    "       # Print the coherence scores\n",
    "        find_first_peak = True\n",
    "        optimal_value = -1\n",
    "        optimal_topics = -1\n",
    "        mapping = list(zip(x, coherence_values))\n",
    "        for i, tup in enumerate(mapping[:-1]):\n",
    "            m, cv = tup\n",
    "\n",
    "            if find_first_peak:\n",
    "                if cv > mapping[i+1][1]:\n",
    "                    find_first_peak = False\n",
    "                optimal_value = cv\n",
    "                optimal_topics = m\n",
    "\n",
    "            if display_output:\n",
    "                print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 4))\n",
    "        if display_output:\n",
    "            print(\"\\nOptimal Topic Count: {}\".format(optimal_topics))\n",
    "\n",
    "        # Build LDA model\n",
    "        lda_model = gensim.models.ldamodel.LdaModel(corpus=tdf_corpus,\n",
    "                                                   id2word=id2word,\n",
    "                                                   num_topics=optimal_topics, \n",
    "                                                   random_state=100,\n",
    "                                                   update_every=1,\n",
    "                                                   chunksize=100,\n",
    "                                                   passes=10,\n",
    "                                                   alpha='auto',\n",
    "                                                   per_word_topics=True)\n",
    "\n",
    "        ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=tdf_corpus, num_topics=optimal_topics, id2word=id2word)\n",
    "\n",
    "        # Visualize the topics\n",
    "        pyLDAvis.enable_notebook()\n",
    "        vis = pyLDAvis.gensim.prepare(lda_model, tdf_corpus, id2word)\n",
    "        path = \"Visualizations/\" + filename + \"/LDA/\" + filename + \"_\" + party + \"party_LDA_visualization.html\"\n",
    "        pyLDAvis.save_html(vis, path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_corpus_LDA_visualizations(corpus, filename, ngram = 1, display_output = False):\n",
    "    \n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    nlp.max_length = 1500000\n",
    "    \n",
    "    text = list(corpus.transcripts.values)\n",
    "\n",
    "    words = list(sent_to_words(text))\n",
    "\n",
    "    # Build the bigram and trigram models\n",
    "    bigram = gensim.models.Phrases(words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "    trigram = gensim.models.Phrases(bigram[words], threshold=100)  \n",
    "\n",
    "    # Faster way to get a sentence clubbed as a trigram/bigram\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "    # Remove Stop Words\n",
    "    words = remove_stopwords(words)\n",
    "\n",
    "    # Form Bigrams\n",
    "    if ngram == 2:\n",
    "        words = make_bigrams(words, bigram_mod)\n",
    "\n",
    "    if ngram == 3:\n",
    "        words = make_trigrams(words, bigram_mod, trigram_mod)\n",
    "\n",
    "    data_lemmatized = lemmatization(words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "    # Create Dictionary\n",
    "    id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "    # Create Corpus\n",
    "    texts = data_lemmatized\n",
    "\n",
    "    # Term Document Frequency\n",
    "    tdf_corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "    # issue\n",
    "    model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=tdf_corpus, texts=data_lemmatized, start=2, limit=40, step=6)\n",
    "\n",
    "    limit=40; start=2; step=6;\n",
    "    x = range(start, limit, step)  \n",
    "\n",
    "   # Print the coherence scores\n",
    "    find_first_peak = True\n",
    "    optimal_value = -1\n",
    "    optimal_topics = -1\n",
    "    mapping = list(zip(x, coherence_values))\n",
    "    for i, tup in enumerate(mapping[:-1]):\n",
    "        m, cv = tup\n",
    "\n",
    "        if find_first_peak:\n",
    "            if cv > mapping[i+1][1]:\n",
    "                find_first_peak = False\n",
    "            optimal_value = cv\n",
    "            optimal_topics = m\n",
    "\n",
    "        if display_output:\n",
    "            print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 4))\n",
    "    if display_output:\n",
    "        print(\"\\nOptimal Topic Count: {}\".format(optimal_topics))\n",
    "\n",
    "    # Build LDA model\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(corpus=tdf_corpus,\n",
    "                                               id2word=id2word,\n",
    "                                               num_topics=optimal_topics, \n",
    "                                               random_state=100,\n",
    "                                               update_every=1,\n",
    "                                               chunksize=100,\n",
    "                                               passes=10,\n",
    "                                               alpha='auto',\n",
    "                                               per_word_topics=True)\n",
    "\n",
    "    ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=tdf_corpus, num_topics=optimal_topics, id2word=id2word)\n",
    "\n",
    "    # Visualize the topics\n",
    "    pyLDAvis.enable_notebook()\n",
    "    vis = pyLDAvis.gensim.prepare(lda_model, tdf_corpus, id2word)\n",
    "    path = \"Visualizations/\" + filename + \"/LDA/\" + filename + \"_LDA_visualization.html\"\n",
    "    pyLDAvis.save_html(vis, path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_scattertext(corpus, cat_col, texts2analyze, html_category, html_label, html_other_label, metadata, filename):\n",
    "        \n",
    "    st_corpus = st.CorpusFromPandas(corpus, \n",
    "                              category_col=cat_col,  \n",
    "                              text_col=texts2analyze, \n",
    "                              nlp=nlp).build()\n",
    "        \n",
    "    html = st.produce_scattertext_explorer(st_corpus,\n",
    "          category= html_category, \n",
    "          category_name= html_label, \n",
    "          not_category_name= html_other_label,\n",
    "          width_in_pixels=1000,\n",
    "          metadata=st_corpus.get_df()[metadata])\n",
    "                                           \n",
    "    path = \"Visualizations/\" + filename + \"/Scattertexts/\" + filename + \"_\" +  html_label + \"_\" + html_other_label + \"_scattertext_visualization.html\"\n",
    "    open(path, 'wb').write(html.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_empath_topics(corpus, cat_col, texts2analyze, html_category, html_label, html_other_label, metadata, filename):\n",
    "    \n",
    "    \n",
    "    feat_builder = st.FeatsFromOnlyEmpath()\n",
    "    \n",
    "    empath_corpus = st.CorpusFromParsedDocuments(corpus,\n",
    "                                             category_col=cat_col,\n",
    "                                             feats_from_spacy_doc=feat_builder,\n",
    "                                             parsed_col=texts2analyze).build()\n",
    "    \n",
    "    html = st.produce_scattertext_explorer(empath_corpus,\n",
    "                                       category=html_category,\n",
    "                                       category_name=html_label,\n",
    "                                       not_category_name=html_other_label,\n",
    "                                        width_in_pixels=1000,\n",
    "                                       metadata=corpus[metadata],\n",
    "                                      use_non_text_features=True,\n",
    "                                       use_full_doc=True,\n",
    "                                       topic_model_term_lists=feat_builder.get_top_model_term_lists())\n",
    "    \n",
    "    path = \"Visualizations/\" + filename + \"/empaths/\" + filename + \"_\" +  html_label + \"_\" + html_other_label + \"_empath_topics_visualization.html\"\n",
    "    open(path, 'wb').write(html.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_term_frequency_visualization(corpus, filename):\n",
    "    \n",
    "    party_pairs = [(a, b) for (a, b) in set(combinations(set(corpus.party.values), 2)) if a != b]\n",
    "    \n",
    "    n = len(party_pairs)\n",
    "    for i, party_pair in enumerate(party_pairs):\n",
    "        party1 = party_pair[0]\n",
    "        party2 = party_pair[1]\n",
    "    \n",
    "        corpus_1 = corpus[corpus.party == party1]\n",
    "        transcript_1 = ' '.join(corpus_1.transcripts.values)\n",
    "\n",
    "        corpus_2 = corpus[corpus.party == party2]\n",
    "        transcript_2 = ' '.join(corpus_2.transcripts.values)\n",
    "\n",
    "        vectorizer = CountVectorizer()\n",
    "        X = vectorizer.fit_transform([transcript_1, transcript_2])\n",
    "\n",
    "        df = pd.DataFrame(data=X.toarray())\n",
    "        df.columns = vectorizer.get_feature_names()\n",
    "        df.index = [party1, party2]\n",
    "        df = df.T\n",
    "\n",
    "        term_cat_freq = st.TermCategoryFrequencies(df)\n",
    "\n",
    "        html = st.produce_scattertext_explorer(\n",
    "            term_cat_freq,\n",
    "            category=party1,\n",
    "            category_name=party1,\n",
    "            not_category_name=party2)\n",
    "\n",
    "        path = \"Visualizations/\" + filename + \"/Term Frequency/\" + filename + \"_\" + party1 + \"_\" + party2 + \"_term_frequency_visualization.html\"\n",
    "        open(path, 'wb').write(html.encode('utf-8'))\n",
    "        \n",
    "        print(\"{}Processed {:.2f}%\".format(' '*(len(filename) + 2), ((i+1)/n)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_list(topic_data):\n",
    "    topic_list = []\n",
    "    for topic_themes in topic_data:\n",
    "        topics = topic_themes[1]\n",
    "        for topic in topics:\n",
    "            topic_list.append(topic[0])\n",
    "    return topic_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize(word, categories):\n",
    "    d = {}\n",
    "    doc1 = nlp(word)\n",
    "    for words in categories:\n",
    "        doc2 = nlp(words)\n",
    "        d[doc1.similarity(doc2)] = words\n",
    "    return d[max(d.keys())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_topic_model(topic_items, categories):\n",
    "    \"\"\" takes a list of list of topics   \"\"\"\n",
    "    topic_models = {}\n",
    "    for category in categories:\n",
    "        topic_models[category] = []\n",
    "    \n",
    "    flat_list = [item for sublist in topic_items for item in sublist]\n",
    "    \n",
    "    for item in topic_items:\n",
    "        tokens = get_topic_list(item)\n",
    "        for token in tokens:\n",
    "            topic_models[categorize(token, categories)].append(token)\n",
    "            \n",
    "    for category in categories:\n",
    "        topic_models[category] = list(set(topic_models[category]))\n",
    "    \n",
    "    return topic_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_topic_frequency_word_similarity(corpus, texts2analyze, cat_col, html_category, html_label, html_other_label, metadata, list_of_topics, filename):\n",
    "    \n",
    "    categories = ['Agricultural', \n",
    "                  'Climate change',\n",
    "                  'Commercial',\n",
    "                  'Cultural',\n",
    "                  'Domestic',\n",
    "                  'Drug reform',\n",
    "                  'Economic',\n",
    "                  'Fiscal',\n",
    "                  'Incomes',\n",
    "                  'Industrial', \n",
    "                  'Investment', \n",
    "                  'Monetary',\n",
    "                  'Tax',\n",
    "                  'Education',\n",
    "                  'Energy',\n",
    "                  'Nuclear energy',\n",
    "                  'Renewable energy', \n",
    "                  'Environmental', \n",
    "                  'Food',\n",
    "                  'Foreign',\n",
    "                  'Health',\n",
    "                  'Pharmaceutical',\n",
    "                  'Vaccination',\n",
    "                  'Housing',\n",
    "                  'Immigration', \n",
    "                  'Knowledge',\n",
    "                  'Language Military',\n",
    "                  'Science',\n",
    "                  'Stem cell',\n",
    "                  'Space',\n",
    "                  'Technology',\n",
    "                   'Social']\n",
    "    \n",
    "    categories = list(map(lambda x: x.lower(), categories))\n",
    "    \n",
    "    topic_models = build_topic_model(list_of_topics, categories)\n",
    "\n",
    "    corpus['parse'] = corpus[texts2analyze].apply(st.whitespace_nlp_with_sentences)\n",
    "\n",
    "    topic_feature_builder = st.FeatsFromTopicModel(topic_models)\n",
    "\n",
    "    topic_corpus = st.CorpusFromParsedDocuments(\n",
    "            corpus,\n",
    "            category_col=cat_col,\n",
    "            parsed_col='parse',\n",
    "            feats_from_spacy_doc=topic_feature_builder\n",
    "    ).build()\n",
    "\n",
    "    html = st.produce_scattertext_explorer(\n",
    "            topic_corpus,\n",
    "            category=html_category,\n",
    "            category_name=html_label,\n",
    "            not_category_name=html_other_label,\n",
    "            width_in_pixels=1000,\n",
    "            metadata=corpus[metadata],\n",
    "            use_non_text_features=True,\n",
    "            use_full_doc=True,\n",
    "            pmi_threshold_coefficient=0,\n",
    "            topic_model_term_lists=topic_feature_builder.get_top_model_term_lists())\n",
    "    \n",
    "    path = \"Visualizations/\" + filename + \"/Topic Frequency/\" + filename + \"_topic_frequency_word_similarity_visualization.html\"\n",
    "    \n",
    "    open(path, 'wb').write(html.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_word_similarity_plot(corpus, texts2analyze, corpus_cat_col, html_category, html_label, html_other_label, metadata, filename):\n",
    "    \n",
    "    corpus['parse'] = corpus[texts2analyze].apply(st.whitespace_nlp_with_sentences)\n",
    "    \n",
    "    st_corpus = (st.CorpusFromParsedDocuments(corpus, category_col=corpus_cat_col, parsed_col='parse')\n",
    "          .build().get_stoplisted_unigram_corpus())\n",
    "    \n",
    "    html = st.produce_projection_explorer(st_corpus, \n",
    "                                          category=html_category, \n",
    "                                          category_name=html_label,\n",
    "                                          not_category_name=html_other_label, \n",
    "                                          metadata=corpus[metadata])\n",
    "    \n",
    "    path = \"Visualizations/\" + filename + \"/Word Similarity/\" + filename + \"_word_similarity_plot_visualization.html\"\n",
    "    open(path, 'wb').write(html.encode('utf-8'))                                                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mallet_path = 'Visualizations/mallet-2.0.8/bin/mallet' # update this path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_corpus(corpus, party1, party2):\n",
    "    return corpus[corpus['party'].isin([party1, party2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_party_pairs(corpus):\n",
    "    return list(combinations(set(corpus.party), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_party_LDA_visualizations(corpus, filename, ngram = 1, display_output = False):\n",
    "    \n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    nlp.max_length = 1500000\n",
    "    \n",
    "    n = len(set(corpus.party))\n",
    "    for party in set(corpus.party):\n",
    "    \n",
    "        text = list(corpus[corpus.party == party].transcripts.values)\n",
    "\n",
    "        words = list(sent_to_words(text))\n",
    "        \n",
    "        # Build the bigram and trigram models\n",
    "        bigram = gensim.models.Phrases(words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "        trigram = gensim.models.Phrases(bigram[words], threshold=100)  \n",
    "\n",
    "        # Faster way to get a sentence clubbed as a trigram/bigram\n",
    "        bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "        trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "        # Remove Stop Words\n",
    "        words = remove_stopwords(words)\n",
    "\n",
    "        # Form Bigrams\n",
    "        if ngram == 2:\n",
    "            words = make_bigrams(words, bigram_mod)\n",
    "\n",
    "        if ngram == 3:\n",
    "            words = make_trigrams(words, bigram_mod, trigram_mod)\n",
    "\n",
    "        data_lemmatized = lemmatization(words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "        # Create Dictionary\n",
    "        id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "        # Create Corpus\n",
    "        texts = data_lemmatized\n",
    "\n",
    "        # Term Document Frequency\n",
    "        tdf_corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "        # issue\n",
    "        model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=tdf_corpus, texts=data_lemmatized, start=2, limit=40, step=6)\n",
    "\n",
    "        limit=40; start=2; step=6;\n",
    "        x = range(start, limit, step)  \n",
    "\n",
    "       # Print the coherence scores\n",
    "        find_first_peak = True\n",
    "        optimal_value = -1\n",
    "        optimal_topics = -1\n",
    "        mapping = list(zip(x, coherence_values))\n",
    "        for i, tup in enumerate(mapping[:-1]):\n",
    "            m, cv = tup\n",
    "\n",
    "            if find_first_peak:\n",
    "                if cv > mapping[i+1][1]:\n",
    "                    find_first_peak = False\n",
    "                optimal_value = cv\n",
    "                optimal_topics = m\n",
    "\n",
    "            if display_output:\n",
    "                print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 4))\n",
    "        if display_output:\n",
    "            print(\"\\nOptimal Topic Count: {}\".format(optimal_topics))\n",
    "\n",
    "        # Build LDA model\n",
    "        lda_model = gensim.models.ldamodel.LdaModel(corpus=tdf_corpus,\n",
    "                                                   id2word=id2word,\n",
    "                                                   num_topics=optimal_topics, \n",
    "                                                   random_state=100,\n",
    "                                                   update_every=1,\n",
    "                                                   chunksize=100,\n",
    "                                                   passes=10,\n",
    "                                                   alpha='auto',\n",
    "                                                   per_word_topics=True)\n",
    "\n",
    "        ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=tdf_corpus, num_topics=optimal_topics, id2word=id2word)\n",
    "\n",
    "        # Visualize the topics\n",
    "        pyLDAvis.enable_notebook()\n",
    "        vis = pyLDAvis.gensim.prepare(lda_model, tdf_corpus, id2word)\n",
    "        path = \"Visualizations/\" + filename + \"/LDA/\" + filename + \"_\" + party + \"party_LDA_visualization.html\"\n",
    "        pyLDAvis.save_html(vis, path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_corpus_visualizations(corpus, corpus_name):\n",
    "    corpus = corpus.reset_index().rename(columns={'index':'presidents', 'Party':'party'})\n",
    "    \n",
    "    n = len(corpus.presidents)\n",
    "    \n",
    "    print(\"{}: Processing presidential speeches...\".format(corpus_name))\n",
    "    presidential_data = {}\n",
    "    for i, president in enumerate(corpus.presidents):\n",
    "        presidential_data[president] = process_presidential_speech(president, corpus, corpus_name, ngram = 2)\n",
    "        print(\"{}Processed {:.2f}%\".format(' '*(len(corpus_name) + 2), ((i+1)/n)*100))\n",
    "      \n",
    "    print(\"{}: Processing topics...\".format(corpus_name))\n",
    "    model_topic_list = []\n",
    "    for i, president in enumerate(corpus.presidents):\n",
    "        model_topic_list.append(presidential_data[president][2])\n",
    "        print(\"{}Processed {:.2f}%\".format(' '*(len(corpus_name) + 2), ((i+1)/n)*100))\n",
    "        \n",
    "    print(\"{}: Generating LDA visualizations for all parties...\".format(corpus_name))\n",
    "    generate_party_LDA_visualizations(corpus, corpus_name, ngram = 2)\n",
    "    \n",
    "    print(\"{}: Generating LDA visualizations for corpus...\".format(corpus_name))\n",
    "    generate_corpus_LDA_visualizations(corpus, corpus_name, ngram = 2)\n",
    "    \n",
    "    print(\"{}: Generating scattertext visualization...\".format(corpus_name))\n",
    "    for i, party_pair in enumerate(generate_party_pairs(corpus)):\n",
    "        html_category = party_pair[0]\n",
    "        html_label = party_pair[0]\n",
    "        html_other_label = party_pair[1]\n",
    "        temp_corpus = filter_corpus(corpus, html_label, html_other_label)\n",
    "        generate_scattertext(temp_corpus, cat_col='party', texts2analyze='transcripts', html_category=html_category, html_label=html_label, html_other_label=html_other_label, metadata='presidents', filename = corpus_name)\n",
    "        print(\"{}Processed {:.2f}%\".format(' '*(len(corpus_name) + 2), ((i+1)/n)*100))\n",
    "\n",
    "    print(\"{}: Generating empath topics visualization...\".format(corpus_name))\n",
    "    for i, party_pair in enumerate(generate_party_pairs(corpus)):\n",
    "        html_category = party_pair[0]\n",
    "        html_label = party_pair[0]\n",
    "        html_other_label = party_pair[1]\n",
    "        temp_corpus = filter_corpus(corpus, html_label, html_other_label)\n",
    "        generate_empath_topics(temp_corpus, cat_col='party', texts2analyze='transcripts', html_category=html_category, html_label=html_label, html_other_label=html_other_label, metadata='presidents', filename = corpus_name)\n",
    "        print(\"{}Processed {:.2f}%\".format(' '*(len(corpus_name) + 2), ((i+1)/n)*100))\n",
    "    \n",
    "    print(\"{}: Generating term frequency visualization...\".format(corpus_name))\n",
    "    generate_term_frequency_visualization(corpus, filename = corpus_name)\n",
    "    \n",
    "    print(\"{}: Generating topic frequency word similarity visualization...\".format(corpus_name))\n",
    "    for i, party_pair in enumerate(generate_party_pairs(corpus)):\n",
    "        html_category = party_pair[0]\n",
    "        html_label = party_pair[0]\n",
    "        html_other_label = party_pair[1]\n",
    "        temp_corpus = filter_corpus(corpus, party_pair[0], party_pair[1])\n",
    "        generate_topic_frequency_word_similarity(temp_corpus, texts2analyze = 'transcripts', cat_col = 'party', html_category = html_category, html_label = html_label, html_other_label = html_other_label, metadata = 'presidents', list_of_topics = model_topic_list, filename = corpus_name)\n",
    "        print(\"{}Processed {:.2f}%\".format(' '*(len(corpus_name) + 2), ((i+1)/n)*100))\n",
    "        \n",
    "    print(\"{}: Generating word similarity visualization...\".format(corpus_name))\n",
    "    for i, party_pair in enumerate(generate_party_pairs(corpus)):\n",
    "        html_category = party_pair[0]\n",
    "        html_label = party_pair[0]\n",
    "        html_other_label = party_pair[1]\n",
    "        temp_corpus = filter_corpus(corpus, party_pair[0], party_pair[1])\n",
    "        generate_word_similarity_plot(temp_corpus, texts2analyze = 'transcripts', corpus_cat_col = 'party', html_category = html_category, html_label = html_label, html_other_label = html_other_label, metadata = 'presidents', filename = corpus_name)\n",
    "        print(\"{}Processed {:.2f}%\".format(' '*(len(corpus_name) + 2), ((i+1)/n)*100))\n",
    "    \n",
    "    print(\"\\nProcess Complete!\\n\\n\".format(corpus_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ncorpus_filenames = [\"first_corpus\", \"second_corpus\", \"third_corpus\", \"fourth_corpus\", \"fifth_corpus\", \"sixth_corpus\"]\\ncorpus_files = [first_corpus, second_corpus, third_corpus, fourth_corpus, fifth_corpus, sixth_corpus]\\ncorpus_list = list(zip(corpus_filenames, corpus_files))\\n\\nfor filename, file in corpus_list:\\n    generate_corpus_visualizations(file, filename)\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_filenames = [\"first_corpus\", \"second_corpus\", \"third_corpus\", \"fourth_corpus\", \"fifth_corpus\", \"sixth_corpus\"]\n",
    "corpus_files = [first_corpus, second_corpus, third_corpus, fourth_corpus, fifth_corpus, sixth_corpus]\n",
    "corpus_list = list(zip(corpus_filenames, corpus_files))\n",
    "\n",
    "for filename, file in corpus_list:\n",
    "    generate_corpus_visualizations(file, filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
